{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m TO_SAVE_MODEL_ARCHITECTURE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m TO_GENERATE_SUMMARIES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     generate_summaries,\n\u001b[1;32m      7\u001b[0m     create_hyperparameter_grid,\n\u001b[1;32m      8\u001b[0m     prepare_data,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, RMSprop, SGD\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define hyperparameter to permutate\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/utils.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DO NOT EDIT. Generated by api_gen.sh\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTypePolicy\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatDTypePolicy\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/api/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/api/activations/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/src/activations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m celu\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/src/activations/activations.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/src/backend/__init__.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Import backend functions.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable \u001b[38;5;28;01mas\u001b[39;00m BackendVariable\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribution_lib\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbuiltins\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf2xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_update_slice\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tensorflow/__init__.py:55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/compat/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/__init__.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py:60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m profiler\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m python_io\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m queue\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/quantization/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.quantization namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_array_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fake_quant_with_min_max_args \u001b[38;5;66;03m# line: 2698\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_array_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fake_quant_with_min_max_args_gradient \u001b[38;5;66;03m# line: 2867\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/compat/v1/quantization/experimental/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.quantization.experimental namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlir\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _QuantizationComponentSpec \u001b[38;5;28;01mas\u001b[39;00m QuantizationComponentSpec \u001b[38;5;66;03m# line: 46\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlir\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _QuantizationMethod \u001b[38;5;28;01mas\u001b[39;00m QuantizationMethod \u001b[38;5;66;03m# line: 42\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlir\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _QuantizationOptions \u001b[38;5;28;01mas\u001b[39;00m QuantizationOptions \u001b[38;5;66;03m# line: 38\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tensorflow/compiler/mlir/quantization/tensorflow/python/quantize_model.py:235\u001b[0m\n\u001b[1;32m    216\u001b[0m   dataset_file_map_serialized \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    217\u001b[0m       signature_key: dataset_file\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m    218\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m signature_key, dataset_file \u001b[38;5;129;01min\u001b[39;00m representative_dataset\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    219\u001b[0m   }\n\u001b[1;32m    220\u001b[0m   pywrap_quantize_model\u001b[38;5;241m.\u001b[39mquantize_ptq_static_range(\n\u001b[1;32m    221\u001b[0m       src_saved_model_path,\n\u001b[1;32m    222\u001b[0m       dst_saved_model_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m       representative_dataset_file_map_serialized\u001b[38;5;241m=\u001b[39mdataset_file_map_serialized,\n\u001b[1;32m    228\u001b[0m   )\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_static_range_quantize\u001b[39m(\n\u001b[1;32m    232\u001b[0m     src_saved_model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    233\u001b[0m     dst_saved_model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    234\u001b[0m     quantization_options: _QuantizationOptions,\n\u001b[0;32m--> 235\u001b[0m     representative_dataset: \u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepr_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRepresentativeDatasetOrMapping\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m autotrackable\u001b[38;5;241m.\u001b[39mAutoTrackable:\n\u001b[1;32m    239\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Quantizes the given SavedModel via static range quantization.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m  If the model is not trained with Quantization-Aware Training (QAT) technique,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m      in the SavedModel.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    266\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    267\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning static range quantization on model: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, src_saved_model_path\n\u001b[1;32m    268\u001b[0m   )\n",
      "File \u001b[0;32m/usr/lib/python3.12/typing.py:395\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/typing.py:510\u001b[0m, in \u001b[0;36m_SpecialForm.__getitem__\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;129m@_tp_cache\u001b[39m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, parameters):\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/typing.py:744\u001b[0m, in \u001b[0;36mOptional\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Optional[X] is equivalent to Union[X, None].\"\"\"\u001b[39;00m\n\u001b[1;32m    743\u001b[0m arg \u001b[38;5;241m=\u001b[39m _type_check(parameters, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires a single type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 744\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/typing.py:395\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/typing.py:510\u001b[0m, in \u001b[0;36m_SpecialForm.__getitem__\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;129m@_tp_cache\u001b[39m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, parameters):\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/typing.py:729\u001b[0m, in \u001b[0;36mUnion\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnionGenericAlias(\u001b[38;5;28mself\u001b[39m, parameters, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_UnionGenericAlias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/typing.py:1248\u001b[0m, in \u001b[0;36m_GenericAlias.__init__\u001b[0;34m(self, origin, args, inst, name)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01mis\u001b[39;00m _TypingEllipsis \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m                       a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[0;32m-> 1248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__parameters__ \u001b[38;5;241m=\u001b[39m \u001b[43m_collect_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m origin\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/typing_extensions.py:3079\u001b[0m, in \u001b[0;36m_collect_parameters\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_unpacked_typevartuple(t):\n\u001b[1;32m   3078\u001b[0m     type_var_tuple_encountered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3079\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__parameters__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m   3081\u001b[0m         parameters\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TO_SAVE_MODEL = True\n",
    "TO_SAVE_MODEL_ARCHITECTURE = True\n",
    "TO_GENERATE_SUMMARIES = True\n",
    "\n",
    "from utils import (\n",
    "    generate_summaries,\n",
    "    create_hyperparameter_grid,\n",
    "    prepare_data,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "# Define hyperparameter to permutate\n",
    "hyperparameter_grid = create_hyperparameter_grid(\n",
    "    embedding_dim=[512],\n",
    "    latent_dim=[256],\n",
    "    encoder_dropout=[0.2],\n",
    "    encoder_recurrent_dropout=[0.2],\n",
    "    decoder_dropout=[0.2],\n",
    "    decoder_recurrent_dropout=[0.2],\n",
    "    optimizers=[{\"class\": Adam, \"learning_rate\": 0.001}],\n",
    "    epochs=[50],\n",
    "    batch_size=[256],\n",
    ")\n",
    "\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "from architectures.Seq2SeqBiLSTMImproved import Seq2SeqBiLSTMImproved\n",
    "\n",
    "# Define models\n",
    "model_classes = [\n",
    "    Seq2SeqLSTMGlove,\n",
    "    # Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqBiLSTM,\n",
    "    # Seq2SeqLSTM,\n",
    "    # Seq2SeqGRU,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "    # Seq2SeqBiLSTMImproved,\n",
    "]\n",
    "\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_metrics_results(df_summaries, model_name, results_path):\n",
    "    metrics_file_path = f\"{results_path}/csv/{model_name}_metrics_scores.csv\"\n",
    "    df_summaries.to_csv(metrics_file_path, index=False)\n",
    "    print(f\"Metrics results saved to {metrics_file_path}\")\n",
    "\n",
    "\n",
    "def plot_training_history(history, model_name, save_path):\n",
    "    plt.plot(history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Model Loss Over Epochs - {model_name}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(\n",
    "        os.path.join(save_path, f\"{model_name}_lossplot.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_model(model, model_name, save_path, save_full_model=True):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # Save the model weights\n",
    "\n",
    "    if save_full_model:\n",
    "        # Save the full model\n",
    "        model.save(os.path.join(save_path, f\"{model_name}_full_model.h5\"))\n",
    "    else:\n",
    "        model.save_weights(os.path.join(save_path, f\"{model_name}.weights.h5\"))\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_instance,\n",
    "    hyperparams,\n",
    "    x_training_padded,\n",
    "    y_training_padded,\n",
    "    x_validation_padded,\n",
    "    y_validation_padded,\n",
    "    save_path,\n",
    "):\n",
    "    K.clear_session()\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    latent_dim = hyperparams[\"latent_dim\"]\n",
    "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
    "    encoder_dropout = hyperparams[\"encoder_dropout\"]\n",
    "    encoder_recurrent_dropout = hyperparams[\"encoder_recurrent_dropout\"]\n",
    "    decoder_dropout = hyperparams[\"decoder_dropout\"]\n",
    "    decoder_recurrent_dropout = hyperparams[\"decoder_recurrent_dropout\"]\n",
    "    optimizer_class = hyperparams[\"optimizer_class\"]\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    learning_rate = hyperparams[\"learning_rate\"]\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "\n",
    "    # Set optimizer and callbacks\n",
    "    model_instance.change_optimizer(optimizer)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    def lr_schedule(epoch, lr):\n",
    "        decay_rate = 0.95\n",
    "        decay_step = 1\n",
    "        if epoch % decay_step == 0 and epoch != 0:\n",
    "            return lr * decay_rate\n",
    "        return lr\n",
    "\n",
    "    learning_rate_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "    # Reduce LR on Plateau\n",
    "    reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    # Add callbacks to the model instance\n",
    "    model_instance.add_callbacks([early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "    model = model_instance.get_model()\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        [x_training_padded, y_training_padded[:, :-1]],\n",
    "        y_training_padded.reshape(\n",
    "            y_training_padded.shape[0], y_training_padded.shape[1], 1\n",
    "        )[:, 1:],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(\n",
    "            [x_validation_padded, y_validation_padded[:, :-1]],\n",
    "            y_validation_padded.reshape(\n",
    "                y_validation_padded.shape[0], y_validation_padded.shape[1], 1\n",
    "            )[:, 1:],\n",
    "        ),\n",
    "        callbacks=model_instance.get_callbacks(),\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    model_name = model_instance.name\n",
    "    model_save_path = os.path.join(save_path, \"weights\")\n",
    "    if TO_SAVE_MODEL:\n",
    "        save_model(model, model_name, model_save_path, save_full_model=False)\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        history.history, model_name, os.path.join(save_path, \"media/graphs\")\n",
    "    )\n",
    "\n",
    "    return history.history\n",
    "\n",
    "\n",
    "# Training loop\n",
    "results_path = f\"results/\"\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "for model_class in model_classes:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Training: {model_class.__name__}\")\n",
    "\n",
    "    results_path = f\"results/{model_class.__name__}\"\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    # Crea the subdirectories\n",
    "    os.makedirs(f\"{results_path}/weights\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/graphs\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/architectures\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/csv\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/histories\", exist_ok=True)\n",
    "\n",
    "    for hyperparams in hyperparameter_grid:\n",
    "        # Get prepared data\n",
    "        (\n",
    "            x_voc,\n",
    "            y_voc,\n",
    "            x_tokenizer,\n",
    "            y_tokenizer,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            max_text_len,\n",
    "            max_summary_len,\n",
    "        ) = prepare_data()\n",
    "\n",
    "        # Name additional information for the instance\n",
    "        additional_info = f\"\"\n",
    "\n",
    "        # Create the model instance\n",
    "        model_instance = model_class(\n",
    "            x_voc=x_voc,\n",
    "            y_voc=y_voc,\n",
    "            max_text_len=max_text_len,\n",
    "            max_summary_len=max_summary_len,\n",
    "            x_tokenizer=x_tokenizer,\n",
    "            y_tokenizer=y_tokenizer,\n",
    "            name_additional_info=f\"{additional_info}_optimizer{hyperparams['optimizer_class'].__name__}_lr{hyperparams['learning_rate']}_ed{hyperparams['embedding_dim']}_ld{hyperparams['latent_dim']}_do{hyperparams['decoder_dropout']}_drdo{hyperparams['decoder_recurrent_dropout']}_edo{hyperparams['encoder_dropout']}_erdo{hyperparams['encoder_recurrent_dropout']}_batch_size{hyperparams['batch_size']}_epochs{hyperparams['epochs']}\",\n",
    "            latent_dim=hyperparams[\"latent_dim\"],\n",
    "            embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "            encoder_dropout=hyperparams[\"encoder_dropout\"],\n",
    "            encoder_recurrent_dropout=hyperparams[\"encoder_recurrent_dropout\"],\n",
    "            decoder_dropout=hyperparams[\"decoder_dropout\"],\n",
    "            decoder_recurrent_dropout=hyperparams[\"decoder_recurrent_dropout\"],\n",
    "        )\n",
    "\n",
    "        # Plot the model architecture\n",
    "        if TO_SAVE_MODEL_ARCHITECTURE:\n",
    "            plot_model(\n",
    "                model_instance.get_model(),\n",
    "                to_file=f\"{results_path}/media/architectures/{model_instance.name}_architecture.png\",\n",
    "                show_shapes=True,\n",
    "            )\n",
    "\n",
    "        print(f\"Training {model_instance.name} with hyperparameters {hyperparams}\")\n",
    "        history = train_model(\n",
    "            model_instance,\n",
    "            hyperparams,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            results_path,\n",
    "        )\n",
    "\n",
    "        # Save training history\n",
    "        history_path = os.path.join(\n",
    "            results_path, f\"histories/{model_instance.name}_history.txt\"\n",
    "        )\n",
    "        with open(history_path, \"a\") as f:\n",
    "            f.write(f\"Hyperparameters: {hyperparams}\\n\")\n",
    "            f.write(f\"History: {history}\\n\")\n",
    "            # Write last epoch loss, val_loss, accuracy, val_accuracy\n",
    "            f.write(\n",
    "                f\"Last epoch loss: {history['loss'][-1]}, val_loss: {history['val_loss'][-1]}, accuracy: {history['accuracy'][-1]}, val_accuracy: {history['val_accuracy'][-1]}\\n\"\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        if TO_GENERATE_SUMMARIES:\n",
    "            # Generate and save summaries\n",
    "            print(f\"Generating summaries for {model_instance.name}\")\n",
    "            summaries_path = os.path.join(results_path, \"csv\")\n",
    "            df_summaries = generate_summaries(\n",
    "                model_instance,\n",
    "                x_training_padded,\n",
    "                y_training_padded,\n",
    "                max_text_len,\n",
    "                n_summaries=1000,\n",
    "                save_path=summaries_path,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 17:00:53.162977: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-03 17:00:53.171280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743692453.180812   18838 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743692453.183676   18838 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-03 17:00:53.194882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2SeqGRU\n",
      "CSV files found (3): ['Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqLSTM\n",
      "CSV files found (3): ['Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqLSTMGlove\n",
      "CSV files found (3): ['Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqBiLSTM\n",
      "CSV files found (3): ['Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2Seq3BiLSTM\n",
      "CSV files found (3): ['Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqGRU\n",
      "Evaluating file: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72862b22c7a74a25863b4ae755d48c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31847f2ff374b0abcebc1a30fceec4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.75 seconds, 1325.44 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c375718b3e024b8b8f9516c775d5a2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907c81392c9d4ca2ac97904281dd586e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.75 seconds, 1329.44 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2609ca2ea64b72b5e01cc7418f1525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044d622b6d1545ef8903fd33b8c9ce9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.76 seconds, 1313.93 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqLSTM\n",
      "Evaluating file: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4835a9124c34caf92b01d91ea62e896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9035be462c854460ba12bad9ea4a27de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.77 seconds, 1303.56 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTM/csv/Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2837563a374c3ca4b0c0e274b63de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214a5055f082433a99fbdb9f74e49074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.78 seconds, 1282.13 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTM/csv/Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ca44a0c6d84de39a2ee5008b6a44f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3310a99b6b9a4895a8ad2882bfb34d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.77 seconds, 1297.66 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTM/csv/Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqLSTMGlove\n",
      "Evaluating file: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c66481ae4a47a9bd00ddc84f9e6584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76455183b5144e0f950f0cb1fba48911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.80 seconds, 1253.60 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTMGlove/csv/Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f5be7198be4a1f9bb8906db57f5d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05daef962551418b85be8728b8824e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.81 seconds, 1240.49 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTMGlove/csv/Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a1c2aeb4204d6bb63aa79b9e4f9743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7da6683aba540bba81ad5b3b6dc2e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.81 seconds, 1235.71 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTMGlove/csv/Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqBiLSTM\n",
      "Evaluating file: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979fb743d65345a29a75e75f267d5312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89592a157f8349b39a0c4cf597abdb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.84 seconds, 1186.19 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqBiLSTM/csv/Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9223f6f7d746d7b9459a73f77796d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669aef9213a846f79c4af7299ebe7eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.85 seconds, 1176.94 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqBiLSTM/csv/Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7180ac77cb44269954e9e34ae104d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccd6c73ec8f40aa9296aaea106f083d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.84 seconds, 1188.52 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqBiLSTM/csv/Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2Seq3BiLSTM\n",
      "Evaluating file: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9605adc2748145f6ab2d3a64960b1929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432a099ed8884715bb072b88c620d332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.75 seconds, 1336.71 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2Seq3BiLSTM/csv/Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8528ada93c0242faa97997d358fd6057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b787f4aae6e943fa984f273ad028d203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.75 seconds, 1334.45 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2Seq3BiLSTM/csv/Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130a748d0da348a0be0d4750a5d8bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af9459b5db548879916a2f63586bf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.74 seconds, 1342.39 sentences/sec\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2Seq3BiLSTM/csv/Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n"
     ]
    }
   ],
   "source": [
    "TO_EVALUATE_SUMMARIES = True\n",
    "TO_SAVE_PLOTS = True\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "from architectures.Seq2SeqBiLSTMImproved import Seq2SeqBiLSTMImproved\n",
    "model_classes = [\n",
    "    Seq2SeqGRU,\n",
    "    Seq2SeqLSTM,\n",
    "    Seq2SeqLSTMGlove,\n",
    "    Seq2SeqBiLSTM,\n",
    "    Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "    # Seq2SeqBiLSTMImproved,\n",
    "]\n",
    "\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    evaluate_rouge,\n",
    "    evaluate_wer,\n",
    "    evaluate_cosine_similarity,\n",
    "    evaluate_myevalutation,\n",
    "    evaluate_bert_score,\n",
    "    plot_rouge,\n",
    "    plot_wer,\n",
    "    plot_cosine_similarity,\n",
    "    plot_myevaluation,\n",
    "    plot_bert_score,\n",
    ")\n",
    "import glob\n",
    "\n",
    "\n",
    "model_instances = {}\n",
    "\n",
    "for model in model_classes:\n",
    "    model_name = str(model.__name__)\n",
    "    csv_dir = os.path.join(\"results\", model_name, \"csv\")\n",
    "\n",
    "    # Find all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "\n",
    "    # Filter only summaries files\n",
    "    summaries_files = [\n",
    "        f for f in csv_files if \"summaries\" in os.path.basename(f).lower()\n",
    "    ]\n",
    "\n",
    "    # If file name contains \"evaluated\", remove it\n",
    "    summaries_files = [\n",
    "        f for f in summaries_files if \"evaluated\" not in os.path.basename(f).lower()\n",
    "    ]\n",
    "\n",
    "    # Extract file names\n",
    "    file_names = [os.path.basename(f) for f in summaries_files]\n",
    "\n",
    "    # Remove duplicates\n",
    "    file_names = list(set(file_names))\n",
    "\n",
    "    model_instances[model] = sorted(file_names)  # Order by name\n",
    "\n",
    "# Print model instances\n",
    "for model, instances in model_instances.items():\n",
    "    print(f\"Model: {model.__name__}\")\n",
    "    print(f\"CSV files found ({len(instances)}): {instances}\\n\")\n",
    "\n",
    "\n",
    "def save_metrics_results(df_summaries, model_name, results_path):\n",
    "    metrics_file_path = f\"{results_path}/csv/{model_name}_metrics_scores.csv\"\n",
    "    df_summaries.to_csv(metrics_file_path, index=False)\n",
    "    print(f\"Metrics results saved to {metrics_file_path}\")\n",
    "\n",
    "\n",
    "if TO_EVALUATE_SUMMARIES:\n",
    "    # Iterate through all models and their instances\n",
    "    for model, instances in model_instances.items():\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Evaluating summaries for {model.__name__}\")\n",
    "        for csv_file in instances:\n",
    "            print(f\"Evaluating file: {csv_file}\")\n",
    "\n",
    "            # Load original csv\n",
    "            original_path = os.path.join(\"results\", model.__name__, \"csv\", csv_file)\n",
    "            df_summaries = pd.read_csv(original_path)\n",
    "\n",
    "            # Evaluate summaries\n",
    "            print(f\"Evaluating rouge\")\n",
    "            df_summaries, mean_scores_rouge = evaluate_rouge(df_summaries)\n",
    "            print(f\"Evaluating wer\")\n",
    "            df_summaries, mean_score_wer = evaluate_wer(df_summaries)\n",
    "            print(f\"Evaluating cosine similarity\")\n",
    "            df_summaries, mean_score_cosine_similarity = evaluate_cosine_similarity(\n",
    "                df_summaries\n",
    "            )\n",
    "            print(f\"Evaluating BERT score\")\n",
    "            df_summaries, mean_score_bert_score = evaluate_bert_score(df_summaries)\n",
    "            print(f\"Evaluating my evaluation\")\n",
    "            df_summaries, mean_score_myevaluation = evaluate_myevalutation(df_summaries)\n",
    "\n",
    "\n",
    "            print(\"Finished evaluation\")\n",
    "\n",
    "            # Create new file name\n",
    "            base_name = os.path.splitext(csv_file)[0]\n",
    "            evaluated_filename = f\"{base_name}_evaluated.csv\"\n",
    "            evaluated_path = os.path.join(\n",
    "                \"results\", model.__name__, \"csv\", evaluated_filename\n",
    "            )\n",
    "\n",
    "            # Save evaluated file\n",
    "            df_summaries.to_csv(evaluated_path, index=False)\n",
    "            print(f\"Evaluated file: {evaluated_path}\")\n",
    "\n",
    "            results_path = f\"results/{model.__name__}\"\n",
    "\n",
    "            # Plotting\n",
    "            if TO_SAVE_PLOTS:\n",
    "                graph_dir = os.path.join(results_path, \"media/graphs\", base_name)\n",
    "                os.makedirs(graph_dir, exist_ok=True)\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rouge1\",\n",
    "                    title=f\"ROUGE-1 - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rouge2\",\n",
    "                    title=f\"ROUGE-2 - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rougeL\",\n",
    "                    title=f\"ROUGE-L - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_wer(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"WER - {base_name}\",\n",
    "                    color=\"red\",\n",
    "                )\n",
    "\n",
    "                plot_cosine_similarity(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"Cosine Similarity - {base_name}\",\n",
    "                    color=\"green\",\n",
    "                )\n",
    "\n",
    "                plot_myevaluation(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"My Evaluation - {base_name}\",\n",
    "                    color=\"purple\",\n",
    "                )\n",
    "\n",
    "                plot_bert_score(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"BERT Score - {base_name}\",\n",
    "                    color=\"orange\",\n",
    "                )\n",
    "\n",
    "            # Update history file\n",
    "            history_path = os.path.join(results_path, f\"histories/{base_name}_history\")\n",
    "            with open(history_path, \"a\") as f:\n",
    "                f.write(f\"\\nEvaluation for {csv_file}:\\n\")\n",
    "                f.write(f\"Mean ROUGE scores: {mean_scores_rouge}\\n\")\n",
    "                f.write(f\"Mean WER score: {mean_score_wer}\\n\")\n",
    "                f.write(f\"Mean Cosine Similarity: {mean_score_cosine_similarity}\\n\")\n",
    "                f.write(f\"Mean My Evaluation: {mean_score_myevaluation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2SeqGRU\n",
      "CSV files found (3): ['Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2SeqLSTM\n",
      "CSV files found (3): ['Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2SeqLSTMGlove\n",
      "CSV files found (3): ['Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2SeqBiLSTM\n",
      "CSV files found (3): ['Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2Seq3BiLSTM\n",
      "CSV files found (3): ['Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "==================================================\n",
      "Processing reports for Seq2SeqGRU\n",
      "Processed: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2SeqLSTM\n",
      "Processed: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2SeqLSTMGlove\n",
      "Processed: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2SeqBiLSTM\n",
      "Processed: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2Seq3BiLSTM\n",
      "Processed: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "Generated ROUGE-1 report: results/evaluations_metrics/evaluation_mean_rouge1.txt\n",
      "Generated ROUGE-2 report: results/evaluations_metrics/evaluation_mean_rouge2.txt\n",
      "Generated ROUGE-L report: results/evaluations_metrics/evaluation_mean_rougeL.txt\n",
      "Generated WER report: results/evaluations_metrics/evaluation_mean_wer.txt\n",
      "Generated Cosine Similarity report: results/evaluations_metrics/evaluation_mean_cosine.txt\n",
      "Generated Custom Evaluation report: results/evaluations_metrics/evaluation_mean_myevaluation.txt\n",
      "Generated BERT Score report: results/evaluations_metrics/evaluation_mean_BERTScore.txt\n",
      "Generated Markdown table report: results/evaluations_metrics/table_report.md\n",
      "| Model - Instance | mean_cosine | mean_myevaluation | mean_BERTScore | mean_wer | mean_rouge1 | mean_rouge2 | mean_rougeL |\n",
      "|---|---|---|---|---|---|---|---|\n",
      "| Seq2Seq3BiLSTM - Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4189 | 0.8151 | 0.8692 | 1.1440 | 0.1564 | 0.0411 | 0.1552 |\n",
      "| Seq2Seq3BiLSTM - Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.4318 | 0.8183 | 0.8714 | 1.1382 | 0.1665 | 0.0435 | 0.1653 |\n",
      "| Seq2Seq3BiLSTM - Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | 0.4198 | 0.8166 | 0.8707 | **1.1611** | 0.1588 | 0.0383 | 0.1583 |\n",
      "| Seq2SeqBiLSTM - Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4631 | 0.8258 | 0.8755 | 1.0647 | 0.2240 | 0.0696 | 0.2227 |\n",
      "| Seq2SeqBiLSTM - Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.4431 | 0.8189 | 0.8700 | 1.0895 | 0.2075 | 0.0575 | 0.2052 |\n",
      "| Seq2SeqBiLSTM - Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | **0.4731** | **0.8290** | **0.8777** | 1.0187 | **0.2416** | **0.0738** | **0.2397** |\n",
      "| Seq2SeqGRU - Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4294 | 0.8192 | 0.8724 | 1.0916 | 0.1656 | 0.0417 | 0.1650 |\n",
      "| Seq2SeqGRU - Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.3899 | 0.8118 | 0.8685 | 1.1170 | 0.1290 | 0.0255 | 0.1287 |\n",
      "| Seq2SeqGRU - Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | 0.4445 | 0.8215 | 0.8733 | 1.1173 | 0.1884 | 0.0563 | 0.1875 |\n",
      "| Seq2SeqLSTM - Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4400 | 0.8218 | 0.8746 | 1.0826 | 0.1702 | 0.0453 | 0.1694 |\n",
      "| Seq2SeqLSTM - Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.4374 | 0.8204 | 0.8734 | 1.0963 | 0.1629 | 0.0407 | 0.1616 |\n",
      "| Seq2SeqLSTM - Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | 0.4266 | 0.8182 | 0.8717 | 1.1331 | 0.1607 | 0.0454 | 0.1604 |\n",
      "| Seq2SeqLSTMGlove - Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4314 | 0.8208 | 0.8739 | 1.0877 | 0.1798 | 0.0472 | 0.1795 |\n",
      "| Seq2SeqLSTMGlove - Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.4467 | 0.8237 | 0.8753 | 1.0803 | 0.1927 | 0.0562 | 0.1918 |\n",
      "| Seq2SeqLSTMGlove - Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | 0.4378 | 0.8219 | 0.8743 | 1.1131 | 0.1861 | 0.0519 | 0.1855 |\n",
      "All reports generated.\n"
     ]
    }
   ],
   "source": [
    "TO_GENERATE_REPORT_SUMMARIES = True\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "from architectures.Seq2SeqBiLSTMImproved import Seq2SeqBiLSTMImproved\n",
    "\n",
    "model_classes = [\n",
    "    Seq2SeqGRU,\n",
    "    Seq2SeqLSTM,\n",
    "    Seq2SeqLSTMGlove,\n",
    "    Seq2SeqBiLSTM,\n",
    "    Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "    # Seq2SeqBiLSTMImproved,\n",
    "]\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    evaluate_rouge,\n",
    "    evaluate_wer,\n",
    "    evaluate_cosine_similarity,\n",
    "    evaluate_myevalutation,\n",
    "    plot_rouge,\n",
    "    plot_wer,\n",
    "    plot_cosine_similarity,\n",
    "    plot_myevaluation,\n",
    ")\n",
    "import ast\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "def generate_metric_reports(all_metrics, results_root=\"results\"):\n",
    "    # Crea directory principale per i report\n",
    "    metrics_dir = os.path.join(results_root, \"evaluations_metrics\")\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "    # Definizione delle metriche e direzioni di ordinamento\n",
    "    metrics_config = {\n",
    "        \"mean_rouge1\": {\"name\": \"ROUGE-1\", \"ascending\": False},\n",
    "        \"mean_rouge2\": {\"name\": \"ROUGE-2\", \"ascending\": False},\n",
    "        \"mean_rougeL\": {\"name\": \"ROUGE-L\", \"ascending\": False},\n",
    "        \"mean_wer\": {\"name\": \"WER\", \"ascending\": True},\n",
    "        \"mean_cosine\": {\"name\": \"Cosine Similarity\", \"ascending\": False},\n",
    "        \"mean_myevaluation\": {\"name\": \"Custom Evaluation\", \"ascending\": False},\n",
    "        \"mean_BERTScore\": {\"name\": \"BERT Score\", \"ascending\": False},\n",
    "    }\n",
    "\n",
    "    for metric, config in metrics_config.items():\n",
    "        # Ordina i risultati\n",
    "        sorted_metrics = sorted(\n",
    "            all_metrics, key=lambda x: x[metric], reverse=not config[\"ascending\"]\n",
    "        )\n",
    "\n",
    "        # Crea contenuto del report\n",
    "        report_content = f\"\"\"\n",
    "{'='*80}\n",
    "{config['name']} Metric Report - Sorted by {config['name']} ({'Descending' if not config['ascending'] else 'Ascending'})\n",
    "{'='*80}\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Total Models/Instances: {len(sorted_metrics)}\n",
    "\n",
    "{'='*80}\n",
    "{\"Rank\":<5} | {\"Model\":<20} | {\"Instance\":<40} | {config['name']:<10} | Other Metrics\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "\n",
    "        for rank, item in enumerate(sorted_metrics, 1):\n",
    "            other_metrics = \", \".join(\n",
    "                [\n",
    "                    f\"{k.split('_')[1].title()}: {v:.4f}\"\n",
    "                    for k, v in item.items()\n",
    "                    if k != \"model\" and k != \"instance\" and k != metric\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            report_content += (\n",
    "                f\"{rank:<5} | {item['model']:<20} | {item['instance']:<40} | \"\n",
    "                f\"{item[metric]:<10.4f} | {other_metrics}\\n\"\n",
    "            )\n",
    "\n",
    "        # Salva il file\n",
    "        report_path = os.path.join(metrics_dir, f\"evaluation_{metric}.txt\")\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "\n",
    "        print(f\"Generated {config['name']} report: {report_path}\")\n",
    "\n",
    "\n",
    "def generate_metric_reports_table(\n",
    "    all_metrics, output_path=\"results/evaluations_metrics/table_report.md\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera una tabella in formato Markdown che riassume le medie di ogni metrica per ogni istanza di modello.\n",
    "\n",
    "    - Le righe sono ordinate alfabeticamente in base al nome del modello e dell'istanza.\n",
    "    - Le colonne corrispondono alle seguenti metriche:\n",
    "        mean_rouge1, mean_rouge2, mean_rougeL, mean_wer, mean_cosine, mean_myevaluation, mean_BERTScore\n",
    "    - In ogni cella viene visualizzata la media della metrica formattata a 4 decimali.\n",
    "    - Per ogni metrica, la cella con il valore massimo viene resa in grassetto.\n",
    "    - Il report viene salvato in output_path.\n",
    "    \"\"\"\n",
    "    # Ordine delle metriche da visualizzare\n",
    "    metric_keys = [\n",
    "        \"mean_cosine\",\n",
    "        \"mean_myevaluation\",\n",
    "        \"mean_BERTScore\",\n",
    "        \"mean_wer\",\n",
    "        \"mean_rouge1\",\n",
    "        \"mean_rouge2\",\n",
    "        \"mean_rougeL\",\n",
    "    ]\n",
    "\n",
    "    # Ordina le righe in base a \"model\" e \"instance\" in ordine alfabetico\n",
    "    sorted_rows = sorted(\n",
    "        all_metrics, key=lambda x: (x[\"model\"].lower(), x[\"instance\"].lower())\n",
    "    )\n",
    "\n",
    "    # Calcola il valore massimo per ogni metrica\n",
    "    max_per_metric = {key: max(row[key] for row in sorted_rows) for key in metric_keys}\n",
    "\n",
    "    # Intestazione della tabella Markdown\n",
    "    header = \"| Model - Instance | \" + \" | \".join(metric_keys) + \" |\"\n",
    "    separator = \"|---|\" + \"|\".join([\"---\"] * len(metric_keys)) + \"|\"\n",
    "\n",
    "    table_lines = [header, separator]\n",
    "\n",
    "    # Aggiungi ogni riga della tabella\n",
    "    for row in sorted_rows:\n",
    "        row_label = f\"{row['model']} - {row['instance']}\"\n",
    "        cell_values = []\n",
    "        for key in metric_keys:\n",
    "            value = row[key]\n",
    "            # Formatta il valore a 4 decimali\n",
    "            formatted_value = f\"{value:.4f}\"\n",
    "            # Se il valore  il massimo per quella metrica, rendilo in grassetto\n",
    "            if value == max_per_metric[key]:\n",
    "                formatted_value = f\"**{formatted_value}**\"\n",
    "            cell_values.append(formatted_value)\n",
    "        table_line = f\"| {row_label} | \" + \" | \".join(cell_values) + \" |\"\n",
    "        table_lines.append(table_line)\n",
    "\n",
    "    # Unione delle righe della tabella\n",
    "    table_md = \"\\n\".join(table_lines)\n",
    "\n",
    "    # Salva la tabella su file\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(table_md)\n",
    "\n",
    "    print(f\"Generated Markdown table report: {output_path}\")\n",
    "    print(table_md)\n",
    "\n",
    "\n",
    "model_instances = {}\n",
    "\n",
    "for model in model_classes:\n",
    "    model_name = str(model.__name__)\n",
    "    csv_dir = os.path.join(\"results\", model_name, \"csv\")\n",
    "\n",
    "    # Find all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "\n",
    "    # Filter only summaries files\n",
    "    summaries_files = [\n",
    "        f for f in csv_files if \"summaries_evaluated\" in os.path.basename(f).lower()\n",
    "    ]\n",
    "\n",
    "    # Extract file names\n",
    "    file_names = [os.path.basename(f) for f in summaries_files]\n",
    "\n",
    "    # Remove duplicates\n",
    "    file_names = list(set(file_names))\n",
    "\n",
    "    model_instances[model] = sorted(file_names)  # Order by name\n",
    "\n",
    "# Print model instances\n",
    "for model, instances in model_instances.items():\n",
    "    print(f\"Model: {model.__name__}\")\n",
    "    print(f\"CSV files found ({len(instances)}): {instances}\\n\")\n",
    "\n",
    "\n",
    "if TO_GENERATE_REPORT_SUMMARIES:\n",
    "    all_metrics = []\n",
    "\n",
    "    for model, instances in model_instances.items():\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Processing reports for {model.__name__}\")\n",
    "\n",
    "        for csv_file in instances:\n",
    "            # Costruisci il percorso completo\n",
    "            csv_path = os.path.join(\"results\", model.__name__, \"csv\", csv_file)\n",
    "\n",
    "            # Carica il CSV\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Funzione per estrarre i valori F1 da rouge_scores\n",
    "            import re\n",
    "\n",
    "            def parse_rouge(row):\n",
    "                try:\n",
    "                    text = row[\"rouge_scores\"]\n",
    "                    # Regex modificata per catturare sia numeri che 'L'\n",
    "                    pattern = r\"rouge([\\dL]+)': Score\\(precision=([\\d\\.]+), recall=([\\d\\.]+), fmeasure=([\\d\\.]+)\\)\"\n",
    "                    matches = re.findall(pattern, text)\n",
    "                    scores = {}\n",
    "                    for m in matches:\n",
    "                        key = f\"rouge{m[0]}\"  # pu essere 'rouge1', 'rouge2' o 'rougeL'\n",
    "                        scores[key] = float(m[3])\n",
    "\n",
    "                    for k in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                        scores.setdefault(k, 0.0)\n",
    "                    return scores\n",
    "                except Exception as e:\n",
    "                    return {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "\n",
    "            # Applicazione della funzione per estrarre i valori\n",
    "            rouge_values = df.apply(parse_rouge, axis=1)\n",
    "\n",
    "            # Calcola le medie\n",
    "            mean_rouge1 = np.mean([v[\"rouge1\"] for v in rouge_values])\n",
    "            mean_rouge2 = np.mean([v[\"rouge2\"] for v in rouge_values])\n",
    "            mean_rougeL = np.mean([v[\"rougeL\"] for v in rouge_values])\n",
    "            mean_wer = df[\"wer_scores\"].mean()\n",
    "            mean_cosine = df[\"cosine_similarity\"].mean()\n",
    "            mean_myevaluation = df[\"myevaluation_scores\"].mean()\n",
    "            mean_BERTScore = df[\"bert_score\"].mean()\n",
    "\n",
    "            # Crea il dizionario delle metriche\n",
    "            metrics_dict = {\n",
    "                \"mean_rouge1\": mean_rouge1,\n",
    "                \"mean_rouge2\": mean_rouge2,\n",
    "                \"mean_rougeL\": mean_rougeL,\n",
    "                \"mean_wer\": mean_wer,\n",
    "                \"mean_cosine\": mean_cosine,\n",
    "                \"mean_myevaluation\": mean_myevaluation,\n",
    "                \"mean_BERTScore\": mean_BERTScore,\n",
    "                \"model\": model.__name__,\n",
    "                \"instance\": os.path.splitext(csv_file)[0].replace(\"_evaluated\", \"\"),\n",
    "            }\n",
    "\n",
    "            all_metrics.append(metrics_dict)\n",
    "            print(f\"Processed: {csv_file}\")\n",
    "\n",
    "    # Genera i report aggregati per metrica\n",
    "    generate_metric_reports(all_metrics)\n",
    "    generate_metric_reports_table(all_metrics)\n",
    "    print(\"All reports generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
