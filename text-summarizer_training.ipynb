{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 12:23:38.818706: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 12:23:39.002014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743503019.052077   24652 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743503019.072733   24652 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 12:23:39.229830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 1\n",
      "\n",
      "==================================================\n",
      "Training: Seq2SeqLSTMGlove\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings already downloaded.\n",
      "Extracting ./architectures/weightsGLOVE/glove.6B.zip...\n",
      "Extraction complete.\n",
      "GloVe embeddings already downloaded.\n",
      "Extracting ./architectures/weightsGLOVE/glove.6B.zip...\n",
      "Extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743503052.975202   24652 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5688 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50 with hyperparameters {'latent_dim': 256, 'embedding_dim': 512, 'encoder_dropout': 0.2, 'encoder_recurrent_dropout': 0.2, 'decoder_dropout': 0.2, 'decoder_recurrent_dropout': 0.2, 'optimizer_class': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'epochs': 50, 'batch_size': 64}\n",
      "Epoch 1/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.5495 - loss: 3.0913 - val_accuracy: 0.6105 - val_loss: 2.4517 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 37ms/step - accuracy: 0.6079 - loss: 2.4347 - val_accuracy: 0.6210 - val_loss: 2.2611 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 37ms/step - accuracy: 0.6162 - loss: 2.2648 - val_accuracy: 0.6272 - val_loss: 2.1726 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 37ms/step - accuracy: 0.6224 - loss: 2.1632 - val_accuracy: 0.6335 - val_loss: 2.1239 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 37ms/step - accuracy: 0.6267 - loss: 2.0914 - val_accuracy: 0.6374 - val_loss: 2.0731 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.6343 - loss: 1.9989 - val_accuracy: 0.6407 - val_loss: 2.0347 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.6372 - loss: 1.9382 - val_accuracy: 0.6439 - val_loss: 2.0086 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 38ms/step - accuracy: 0.6401 - loss: 1.8784 - val_accuracy: 0.6457 - val_loss: 1.9973 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.6461 - loss: 1.8206 - val_accuracy: 0.6453 - val_loss: 1.9867 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 37ms/step - accuracy: 0.6495 - loss: 1.7772 - val_accuracy: 0.6457 - val_loss: 1.9768 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.6529 - loss: 1.7311 - val_accuracy: 0.6481 - val_loss: 1.9794 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.6557 - loss: 1.6921 - val_accuracy: 0.6476 - val_loss: 1.9752 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 40ms/step - accuracy: 0.6599 - loss: 1.6487 - val_accuracy: 0.6492 - val_loss: 1.9826 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m645/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6632 - loss: 1.6099\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.6632 - loss: 1.6099 - val_accuracy: 0.6484 - val_loss: 1.9842 - learning_rate: 0.0010\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Generating summaries for Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50\n",
      "Loading summaries from results/Seq2SeqLSTMGlove/csv/Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv...\n",
      "                                                                                                                                                    original_text  \\\n",
      "0                     daughter used quaker instant oatmeal liked lot long time ago tried version like fact want finish sure maybe lots new ingredients artificial   \n",
      "1                                                                                                                  liked coffee much subscribing dark rich smooth   \n",
      "2  like fool ordered box instead bag try disgusting smell like dead fish rinsed rinsed still smelled right light even look purple crack cannot believe stupid buy   \n",
      "3                                                        flavorful packaged soup long time nice spicy taste good blend herbs cannot wait becomes available stores   \n",
      "4                                                                                      taste like pop refreshing mouth liked healthier pop great taste refreshing   \n",
      "\n",
      "          original_summary         predicted_summary  \n",
      "0            not very good   not as good as expected  \n",
      "1  makes great cup of java              great coffee  \n",
      "2                    gross                     gross  \n",
      "3          delicious treat                great soup  \n",
      "4                very good              great flavor  \n",
      "\n",
      "==================================================\n",
      "Training: Seq2Seq3BiLSTM\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50 with hyperparameters {'latent_dim': 256, 'embedding_dim': 512, 'encoder_dropout': 0.2, 'encoder_recurrent_dropout': 0.2, 'decoder_dropout': 0.2, 'decoder_recurrent_dropout': 0.2, 'optimizer_class': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'epochs': 50, 'batch_size': 64}\n",
      "Epoch 1/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 75ms/step - accuracy: 0.5743 - loss: 2.9073 - val_accuracy: 0.6215 - val_loss: 2.3159 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.6243 - loss: 2.2489 - val_accuracy: 0.6379 - val_loss: 2.1441 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.6396 - loss: 2.0573 - val_accuracy: 0.6442 - val_loss: 2.0647 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.6474 - loss: 1.9252 - val_accuracy: 0.6488 - val_loss: 2.0154 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.6575 - loss: 1.7961 - val_accuracy: 0.6520 - val_loss: 1.9873 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 75ms/step - accuracy: 0.6693 - loss: 1.6786 - val_accuracy: 0.6510 - val_loss: 1.9700 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6777 - loss: 1.5713 - val_accuracy: 0.6522 - val_loss: 1.9711 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6908 - loss: 1.4645\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.6908 - loss: 1.4646 - val_accuracy: 0.6535 - val_loss: 1.9825 - learning_rate: 0.0010\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Generating summaries for Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50\n",
      "Loading summaries from results/Seq2Seq3BiLSTM/csv/Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv...\n",
      "                                                                                                                                                    original_text  \\\n",
      "0                     daughter used quaker instant oatmeal liked lot long time ago tried version like fact want finish sure maybe lots new ingredients artificial   \n",
      "1                                                                                                                  liked coffee much subscribing dark rich smooth   \n",
      "2  like fool ordered box instead bag try disgusting smell like dead fish rinsed rinsed still smelled right light even look purple crack cannot believe stupid buy   \n",
      "3                                                        flavorful packaged soup long time nice spicy taste good blend herbs cannot wait becomes available stores   \n",
      "4                                                                                      taste like pop refreshing mouth liked healthier pop great taste refreshing   \n",
      "\n",
      "          original_summary predicted_summary  \n",
      "0            not very good           not bad  \n",
      "1  makes great cup of java      great coffee  \n",
      "2                    gross   not as pictured  \n",
      "3          delicious treat     great product  \n",
      "4                very good       great taste  \n",
      "\n",
      "==================================================\n",
      "Training: Seq2SeqBiLSTM\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50 with hyperparameters {'latent_dim': 256, 'embedding_dim': 512, 'encoder_dropout': 0.2, 'encoder_recurrent_dropout': 0.2, 'decoder_dropout': 0.2, 'decoder_recurrent_dropout': 0.2, 'optimizer_class': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'epochs': 50, 'batch_size': 64}\n",
      "Epoch 1/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.5685 - loss: 2.9493 - val_accuracy: 0.6302 - val_loss: 2.2033 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.6329 - loss: 2.1324 - val_accuracy: 0.6489 - val_loss: 1.9989 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.6554 - loss: 1.8429 - val_accuracy: 0.6533 - val_loss: 1.9250 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.6739 - loss: 1.6253 - val_accuracy: 0.6550 - val_loss: 1.9089 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 34ms/step - accuracy: 0.6979 - loss: 1.4201 - val_accuracy: 0.6550 - val_loss: 1.9380 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m645/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7266 - loss: 1.2133\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.7266 - loss: 1.2135 - val_accuracy: 0.6503 - val_loss: 2.0040 - learning_rate: 0.0010\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Generating summaries for Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50\n",
      "Loading summaries from results/Seq2SeqBiLSTM/csv/Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv...\n",
      "                                                                                                                                                    original_text  \\\n",
      "0                     daughter used quaker instant oatmeal liked lot long time ago tried version like fact want finish sure maybe lots new ingredients artificial   \n",
      "1                                                                                                                  liked coffee much subscribing dark rich smooth   \n",
      "2  like fool ordered box instead bag try disgusting smell like dead fish rinsed rinsed still smelled right light even look purple crack cannot believe stupid buy   \n",
      "3                                                        flavorful packaged soup long time nice spicy taste good blend herbs cannot wait becomes available stores   \n",
      "4                                                                                      taste like pop refreshing mouth liked healthier pop great taste refreshing   \n",
      "\n",
      "          original_summary predicted_summary  \n",
      "0            not very good        good stuff  \n",
      "1  makes great cup of java      great coffee  \n",
      "2                    gross           not bad  \n",
      "3          delicious treat       good flavor  \n",
      "4                very good       great taste  \n",
      "\n",
      "==================================================\n",
      "Training: Seq2SeqLSTM\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50 with hyperparameters {'latent_dim': 256, 'embedding_dim': 512, 'encoder_dropout': 0.2, 'encoder_recurrent_dropout': 0.2, 'decoder_dropout': 0.2, 'decoder_recurrent_dropout': 0.2, 'optimizer_class': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'epochs': 50, 'batch_size': 64}\n",
      "Epoch 1/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 42ms/step - accuracy: 0.5641 - loss: 3.0402 - val_accuracy: 0.6156 - val_loss: 2.3937 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.6151 - loss: 2.3716 - val_accuracy: 0.6294 - val_loss: 2.2211 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 44ms/step - accuracy: 0.6330 - loss: 2.1467 - val_accuracy: 0.6395 - val_loss: 2.0911 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 44ms/step - accuracy: 0.6430 - loss: 1.9935 - val_accuracy: 0.6449 - val_loss: 2.0264 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.6509 - loss: 1.8718 - val_accuracy: 0.6476 - val_loss: 1.9942 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.6583 - loss: 1.7671 - val_accuracy: 0.6512 - val_loss: 1.9725 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.6657 - loss: 1.6805 - val_accuracy: 0.6516 - val_loss: 1.9693 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 41ms/step - accuracy: 0.6737 - loss: 1.5915 - val_accuracy: 0.6515 - val_loss: 1.9785 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6823 - loss: 1.5113\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 41ms/step - accuracy: 0.6823 - loss: 1.5113 - val_accuracy: 0.6493 - val_loss: 1.9940 - learning_rate: 0.0010\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Generating summaries for Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50\n",
      "Loading summaries from results/Seq2SeqLSTM/csv/Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv...\n",
      "                                                                                                                                                    original_text  \\\n",
      "0                     daughter used quaker instant oatmeal liked lot long time ago tried version like fact want finish sure maybe lots new ingredients artificial   \n",
      "1                                                                                                                  liked coffee much subscribing dark rich smooth   \n",
      "2  like fool ordered box instead bag try disgusting smell like dead fish rinsed rinsed still smelled right light even look purple crack cannot believe stupid buy   \n",
      "3                                                        flavorful packaged soup long time nice spicy taste good blend herbs cannot wait becomes available stores   \n",
      "4                                                                                      taste like pop refreshing mouth liked healthier pop great taste refreshing   \n",
      "\n",
      "          original_summary predicted_summary  \n",
      "0            not very good          not good  \n",
      "1  makes great cup of java      great coffee  \n",
      "2                    gross              yuck  \n",
      "3          delicious treat        great soup  \n",
      "4                very good       great taste  \n",
      "\n",
      "==================================================\n",
      "Training: Seq2SeqGRU\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50 with hyperparameters {'latent_dim': 256, 'embedding_dim': 512, 'encoder_dropout': 0.2, 'encoder_recurrent_dropout': 0.2, 'decoder_dropout': 0.2, 'decoder_recurrent_dropout': 0.2, 'optimizer_class': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'epochs': 50, 'batch_size': 64}\n",
      "Epoch 1/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 50ms/step - accuracy: 0.5756 - loss: 2.9221 - val_accuracy: 0.6299 - val_loss: 2.2632 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 47ms/step - accuracy: 0.6295 - loss: 2.1862 - val_accuracy: 0.6392 - val_loss: 2.1152 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 47ms/step - accuracy: 0.6429 - loss: 1.9911 - val_accuracy: 0.6490 - val_loss: 1.9981 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 48ms/step - accuracy: 0.6561 - loss: 1.8181 - val_accuracy: 0.6500 - val_loss: 1.9720 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 47ms/step - accuracy: 0.6635 - loss: 1.7214 - val_accuracy: 0.6528 - val_loss: 1.9431 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 49ms/step - accuracy: 0.6749 - loss: 1.6116 - val_accuracy: 0.6508 - val_loss: 1.9516 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m645/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6805 - loss: 1.5390\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m646/646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 49ms/step - accuracy: 0.6805 - loss: 1.5391 - val_accuracy: 0.6496 - val_loss: 1.9556 - learning_rate: 0.0010\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Generating summaries for Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50\n",
      "Loading summaries from results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv...\n",
      "                                                                                                                                                    original_text  \\\n",
      "0                     daughter used quaker instant oatmeal liked lot long time ago tried version like fact want finish sure maybe lots new ingredients artificial   \n",
      "1                                                                                                                  liked coffee much subscribing dark rich smooth   \n",
      "2  like fool ordered box instead bag try disgusting smell like dead fish rinsed rinsed still smelled right light even look purple crack cannot believe stupid buy   \n",
      "3                                                        flavorful packaged soup long time nice spicy taste good blend herbs cannot wait becomes available stores   \n",
      "4                                                                                      taste like pop refreshing mouth liked healthier pop great taste refreshing   \n",
      "\n",
      "          original_summary      predicted_summary  \n",
      "0            not very good   not as good as hoped  \n",
      "1  makes great cup of java           great coffee  \n",
      "2                    gross      not what expected  \n",
      "3          delicious treat             good stuff  \n",
      "4                very good            great taste  \n"
     ]
    }
   ],
   "source": [
    "TO_SAVE_MODEL = True\n",
    "TO_SAVE_MODEL_ARCHITECTURE = True\n",
    "TO_GENERATE_SUMMARIES = True\n",
    "\n",
    "from utils import (\n",
    "    generate_summaries,\n",
    "    create_hyperparameter_grid,\n",
    "    prepare_data,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "# Define hyperparameter to permutate\n",
    "hyperparameter_grid = create_hyperparameter_grid(\n",
    "    embedding_dim=[512],\n",
    "    latent_dim=[256],\n",
    "    encoder_dropout=[0.2],\n",
    "    encoder_recurrent_dropout=[0.2],\n",
    "    decoder_dropout=[0.2],\n",
    "    decoder_recurrent_dropout=[0.2],\n",
    "    optimizers=[{\"class\": Adam, \"learning_rate\": 0.001}],\n",
    "    epochs=[50],\n",
    "    batch_size=[64],\n",
    ")\n",
    "\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "from architectures.Seq2SeqBiLSTMImproved import Seq2SeqBiLSTMImproved\n",
    "\n",
    "# Define models\n",
    "model_classes = [\n",
    "    Seq2SeqLSTMGlove,\n",
    "    Seq2Seq3BiLSTM,\n",
    "    Seq2SeqBiLSTM,\n",
    "    Seq2SeqLSTM,\n",
    "    Seq2SeqGRU,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "    # Seq2SeqBiLSTMImproved,\n",
    "]\n",
    "\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def save_metrics_results(df_summaries, model_name, results_path):\n",
    "    metrics_file_path = f\"{results_path}/csv/{model_name}_metrics_scores.csv\"\n",
    "    df_summaries.to_csv(metrics_file_path, index=False)\n",
    "    print(f\"Metrics results saved to {metrics_file_path}\")\n",
    "\n",
    "\n",
    "def plot_training_history(history, model_name, save_path):\n",
    "    plt.plot(history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Model Loss Over Epochs - {model_name}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(\n",
    "        os.path.join(save_path, f\"{model_name}_lossplot.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_model(model, model_name, save_path, save_full_model=True):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # Save the model weights\n",
    "    \n",
    "    if save_full_model:\n",
    "        # Save the full model\n",
    "        model.save(os.path.join(save_path, f\"{model_name}_full_model.h5\"))\n",
    "    else:\n",
    "        model.save_weights(os.path.join(save_path, f\"{model_name}.weights.h5\"))\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_instance,\n",
    "    hyperparams,\n",
    "    x_training_padded,\n",
    "    y_training_padded,\n",
    "    x_validation_padded,\n",
    "    y_validation_padded,\n",
    "    save_path,\n",
    "):\n",
    "    K.clear_session()\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    latent_dim = hyperparams[\"latent_dim\"]\n",
    "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
    "    encoder_dropout = hyperparams[\"encoder_dropout\"]\n",
    "    encoder_recurrent_dropout = hyperparams[\"encoder_recurrent_dropout\"]\n",
    "    decoder_dropout = hyperparams[\"decoder_dropout\"]\n",
    "    decoder_recurrent_dropout = hyperparams[\"decoder_recurrent_dropout\"]\n",
    "    optimizer_class = hyperparams[\"optimizer_class\"]\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    learning_rate = hyperparams[\"learning_rate\"]\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "\n",
    "    # Set optimizer and callbacks\n",
    "    model_instance.change_optimizer(optimizer)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    def lr_schedule(epoch, lr):\n",
    "        decay_rate = 0.95\n",
    "        decay_step = 1\n",
    "        if epoch % decay_step == 0 and epoch != 0:\n",
    "            return lr * decay_rate\n",
    "        return lr\n",
    "\n",
    "    learning_rate_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "    # Reduce LR on Plateau\n",
    "    reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    # Add callbacks to the model instance\n",
    "    model_instance.add_callbacks(\n",
    "        [early_stopping, reduce_lr_on_plateau]\n",
    "    )\n",
    "\n",
    "    model = model_instance.get_model()\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        [x_training_padded, y_training_padded[:, :-1]],\n",
    "        y_training_padded.reshape(\n",
    "            y_training_padded.shape[0], y_training_padded.shape[1], 1\n",
    "        )[:, 1:],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(\n",
    "            [x_validation_padded, y_validation_padded[:, :-1]],\n",
    "            y_validation_padded.reshape(\n",
    "                y_validation_padded.shape[0], y_validation_padded.shape[1], 1\n",
    "            )[:, 1:],\n",
    "        ),\n",
    "        callbacks=model_instance.get_callbacks(),\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    model_name = model_instance.name\n",
    "    model_save_path = os.path.join(save_path, \"weights\")\n",
    "    if TO_SAVE_MODEL:\n",
    "        save_model(model, model_name, model_save_path, save_full_model=False)\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        history.history, model_name, os.path.join(save_path, \"media/graphs\")\n",
    "    )\n",
    "\n",
    "    return history.history\n",
    "\n",
    "\n",
    "# Training loop\n",
    "results_path = f\"results/\"\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "for model_class in model_classes:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Training: {model_class.__name__}\")\n",
    "\n",
    "    results_path = f\"results/{model_class.__name__}\"\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    # Crea the subdirectories\n",
    "    os.makedirs(f\"{results_path}/weights\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/graphs\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/architectures\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/csv\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/histories\", exist_ok=True)\n",
    "\n",
    "    for hyperparams in hyperparameter_grid:\n",
    "        # Get prepared data\n",
    "        (\n",
    "            x_voc,\n",
    "            y_voc,\n",
    "            x_tokenizer,\n",
    "            y_tokenizer,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            max_text_len,\n",
    "            max_summary_len,\n",
    "        ) = prepare_data()\n",
    "\n",
    "        # Name additional information for the instance\n",
    "        additional_info = f\"\"\n",
    "\n",
    "        # Create the model instance\n",
    "        model_instance = model_class(\n",
    "            x_voc=x_voc,\n",
    "            y_voc=y_voc,\n",
    "            max_text_len=max_text_len,\n",
    "            max_summary_len=max_summary_len,\n",
    "            x_tokenizer=x_tokenizer,\n",
    "            y_tokenizer=y_tokenizer,\n",
    "            name_additional_info=f\"{additional_info}_optimizer{hyperparams['optimizer_class'].__name__}_lr{hyperparams['learning_rate']}_ed{hyperparams['embedding_dim']}_ld{hyperparams['latent_dim']}_do{hyperparams['decoder_dropout']}_drdo{hyperparams['decoder_recurrent_dropout']}_edo{hyperparams['encoder_dropout']}_erdo{hyperparams['encoder_recurrent_dropout']}_batch_size{hyperparams['batch_size']}_epochs{hyperparams['epochs']}\",\n",
    "            latent_dim=hyperparams[\"latent_dim\"],\n",
    "            embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "            encoder_dropout=hyperparams[\"encoder_dropout\"],\n",
    "            encoder_recurrent_dropout=hyperparams[\"encoder_recurrent_dropout\"],\n",
    "            decoder_dropout=hyperparams[\"decoder_dropout\"],\n",
    "            decoder_recurrent_dropout=hyperparams[\"decoder_recurrent_dropout\"],\n",
    "        )\n",
    "\n",
    "        # Plot the model architecture\n",
    "        if TO_SAVE_MODEL_ARCHITECTURE:\n",
    "            plot_model(\n",
    "                model_instance.get_model(),\n",
    "                to_file=f\"{results_path}/media/architectures/{model_instance.name}_architecture.png\",\n",
    "                show_shapes=True,\n",
    "            )\n",
    "\n",
    "        print(f\"Training {model_instance.name} with hyperparameters {hyperparams}\")\n",
    "        history = train_model(\n",
    "            model_instance,\n",
    "            hyperparams,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            results_path,\n",
    "        )\n",
    "\n",
    "        # Save training history\n",
    "        history_path = os.path.join(\n",
    "            results_path, f\"histories/{model_instance.name}_history.txt\"\n",
    "        )\n",
    "        with open(history_path, \"a\") as f:\n",
    "            f.write(f\"Hyperparameters: {hyperparams}\\n\")\n",
    "            f.write(f\"History: {history}\\n\")\n",
    "            # Write last epoch loss, val_loss, accuracy, val_accuracy\n",
    "            f.write(\n",
    "                f\"Last epoch loss: {history['loss'][-1]}, val_loss: {history['val_loss'][-1]}\\n\"\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        if TO_GENERATE_SUMMARIES:\n",
    "            # Generate and save summaries\n",
    "            print(f\"Generating summaries for {model_instance.name}\")\n",
    "            summaries_path = os.path.join(results_path, \"csv\")\n",
    "            df_summaries = generate_summaries(\n",
    "                model_instance,\n",
    "                x_training_padded,\n",
    "                y_training_padded,\n",
    "                max_text_len,\n",
    "                n_summaries=1000,\n",
    "                save_path=summaries_path,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2SeqGRU\n",
      "CSV files found (3): ['Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqLSTM\n",
      "CSV files found (3): ['Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqLSTMGlove\n",
      "CSV files found (3): ['Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqBiLSTM\n",
      "CSV files found (3): ['Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2Seq3BiLSTM\n",
      "CSV files found (3): ['Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv', 'Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv', 'Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqBiLSTMImproved\n",
      "CSV files found (1): ['Seq2SeqBiLSTMImproved_optimizerAdam_lr0.001_ed300_ld256_do0.3_drdo0.3_edo0.3_erdo0.3_batch_size64_epochs50_summaries.csv']\n",
      "\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqGRU\n",
      "Evaluating file: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqLSTM\n",
      "Evaluating file: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTM/csv/Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTM/csv/Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTM/csv/Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqLSTMGlove\n",
      "Evaluating file: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTMGlove/csv/Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTMGlove/csv/Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTMGlove/csv/Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqBiLSTM\n",
      "Evaluating file: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqBiLSTM/csv/Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqBiLSTM/csv/Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqBiLSTM/csv/Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2Seq3BiLSTM\n",
      "Evaluating file: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2Seq3BiLSTM/csv/Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2Seq3BiLSTM/csv/Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Evaluating file: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2Seq3BiLSTM/csv/Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqBiLSTMImproved\n",
      "Evaluating file: Seq2SeqBiLSTMImproved_optimizerAdam_lr0.001_ed300_ld256_do0.3_drdo0.3_edo0.3_erdo0.3_batch_size64_epochs50_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqBiLSTMImproved/csv/Seq2SeqBiLSTMImproved_optimizerAdam_lr0.001_ed300_ld256_do0.3_drdo0.3_edo0.3_erdo0.3_batch_size64_epochs50_summaries_evaluated.csv\n"
     ]
    }
   ],
   "source": [
    "TO_EVALUATE_SUMMARIES = True\n",
    "TO_SAVE_PLOTS = True\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "from architectures.Seq2SeqBiLSTMImproved import Seq2SeqBiLSTMImproved\n",
    "model_classes = [\n",
    "    Seq2SeqGRU,\n",
    "    Seq2SeqLSTM,\n",
    "    Seq2SeqLSTMGlove,\n",
    "    Seq2SeqBiLSTM,\n",
    "    Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "    Seq2SeqBiLSTMImproved,\n",
    "]\n",
    "\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    evaluate_rouge,\n",
    "    evaluate_wer,\n",
    "    evaluate_cosine_similarity,\n",
    "    evaluate_myevalutation,\n",
    "    plot_rouge,\n",
    "    plot_wer,\n",
    "    plot_cosine_similarity,\n",
    "    plot_myevaluation,\n",
    ")\n",
    "import glob\n",
    "\n",
    "\n",
    "model_instances = {}\n",
    "\n",
    "for model in model_classes:\n",
    "    model_name = str(model.__name__)\n",
    "    csv_dir = os.path.join(\"results\", model_name, \"csv\")\n",
    "\n",
    "    # Find all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "\n",
    "    # Filter only summaries files\n",
    "    summaries_files = [\n",
    "        f for f in csv_files if \"summaries\" in os.path.basename(f).lower()\n",
    "    ]\n",
    "\n",
    "    # If file name contains \"evaluated\", remove it\n",
    "    summaries_files = [\n",
    "        f for f in summaries_files if \"evaluated\" not in os.path.basename(f).lower()\n",
    "    ]\n",
    "\n",
    "    # Extract file names\n",
    "    file_names = [os.path.basename(f) for f in summaries_files]\n",
    "\n",
    "    # Remove duplicates\n",
    "    file_names = list(set(file_names))\n",
    "\n",
    "    model_instances[model] = sorted(file_names)  # Order by name\n",
    "\n",
    "# Print model instances\n",
    "for model, instances in model_instances.items():\n",
    "    print(f\"Model: {model.__name__}\")\n",
    "    print(f\"CSV files found ({len(instances)}): {instances}\\n\")\n",
    "\n",
    "\n",
    "def save_metrics_results(df_summaries, model_name, results_path):\n",
    "    metrics_file_path = f\"{results_path}/csv/{model_name}_metrics_scores.csv\"\n",
    "    df_summaries.to_csv(metrics_file_path, index=False)\n",
    "    print(f\"Metrics results saved to {metrics_file_path}\")\n",
    "\n",
    "\n",
    "if TO_EVALUATE_SUMMARIES:\n",
    "    # Iterate through all models and their instances\n",
    "    for model, instances in model_instances.items():\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Evaluating summaries for {model.__name__}\")\n",
    "        for csv_file in instances:\n",
    "            print(f\"Evaluating file: {csv_file}\")\n",
    "\n",
    "            # Load original csv\n",
    "            original_path = os.path.join(\"results\", model.__name__, \"csv\", csv_file)\n",
    "            df_summaries = pd.read_csv(original_path)\n",
    "\n",
    "            # Evaluate summaries\n",
    "            print(f\"Evaluating rouge\")\n",
    "            df_summaries, mean_scores_rouge = evaluate_rouge(df_summaries)\n",
    "            print(f\"Evaluating wer\")\n",
    "            df_summaries, mean_score_wer = evaluate_wer(df_summaries)\n",
    "            print(f\"Evaluating cosine similarity\")\n",
    "            df_summaries, mean_score_cosine_similarity = evaluate_cosine_similarity(\n",
    "                df_summaries\n",
    "            )\n",
    "            print(f\"Evaluating my evaluation\")\n",
    "            df_summaries, mean_score_myevaluation = evaluate_myevalutation(df_summaries)\n",
    "\n",
    "            print(\"Finished evaluation\")\n",
    "\n",
    "            # Create new file name\n",
    "            base_name = os.path.splitext(csv_file)[0]\n",
    "            evaluated_filename = f\"{base_name}_evaluated.csv\"\n",
    "            evaluated_path = os.path.join(\n",
    "                \"results\", model.__name__, \"csv\", evaluated_filename\n",
    "            )\n",
    "\n",
    "            # Save evaluated file\n",
    "            df_summaries.to_csv(evaluated_path, index=False)\n",
    "            print(f\"Evaluated file: {evaluated_path}\")\n",
    "\n",
    "            results_path = f\"results/{model.__name__}\"\n",
    "\n",
    "            # Plotting\n",
    "            if TO_SAVE_PLOTS:\n",
    "                graph_dir = os.path.join(results_path, \"media/graphs\", base_name)\n",
    "                os.makedirs(graph_dir, exist_ok=True)\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rouge1\",\n",
    "                    title=f\"ROUGE-1 - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rouge2\",\n",
    "                    title=f\"ROUGE-2 - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rougeL\",\n",
    "                    title=f\"ROUGE-L - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_wer(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"WER - {base_name}\",\n",
    "                    color=\"red\",\n",
    "                )\n",
    "\n",
    "                plot_cosine_similarity(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"Cosine Similarity - {base_name}\",\n",
    "                    color=\"green\",\n",
    "                )\n",
    "\n",
    "                plot_myevaluation(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"My Evaluation - {base_name}\",\n",
    "                    color=\"purple\",\n",
    "                )\n",
    "\n",
    "            # Update history file\n",
    "            history_path = os.path.join(results_path, f\"histories/{base_name}_history\")\n",
    "            with open(history_path, \"a\") as f:\n",
    "                f.write(f\"\\nEvaluation for {csv_file}:\\n\")\n",
    "                f.write(f\"Mean ROUGE scores: {mean_scores_rouge}\\n\")\n",
    "                f.write(f\"Mean WER score: {mean_score_wer}\\n\")\n",
    "                f.write(f\"Mean Cosine Similarity: {mean_score_cosine_similarity}\\n\")\n",
    "                f.write(f\"Mean My Evaluation: {mean_score_myevaluation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2SeqGRU\n",
      "CSV files found (3): ['Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2SeqLSTM\n",
      "CSV files found (3): ['Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2SeqLSTMGlove\n",
      "CSV files found (3): ['Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2SeqBiLSTM\n",
      "CSV files found (3): ['Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2Seq3BiLSTM\n",
      "CSV files found (3): ['Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv', 'Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv', 'Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2SeqBiLSTMImproved\n",
      "CSV files found (1): ['Seq2SeqBiLSTMImproved_optimizerAdam_lr0.001_ed300_ld256_do0.3_drdo0.3_edo0.3_erdo0.3_batch_size64_epochs50_summaries_evaluated.csv']\n",
      "\n",
      "==================================================\n",
      "Processing reports for Seq2SeqGRU\n",
      "Processed: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2SeqLSTM\n",
      "Processed: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2SeqLSTMGlove\n",
      "Processed: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2SeqBiLSTM\n",
      "Processed: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2Seq3BiLSTM\n",
      "Processed: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries_evaluated.csv\n",
      "Processed: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries_evaluated.csv\n",
      "==================================================\n",
      "Processing reports for Seq2SeqBiLSTMImproved\n",
      "Processed: Seq2SeqBiLSTMImproved_optimizerAdam_lr0.001_ed300_ld256_do0.3_drdo0.3_edo0.3_erdo0.3_batch_size64_epochs50_summaries_evaluated.csv\n",
      "Generated ROUGE-1 report: results/evaluations_metrics/evaluation_mean_rouge1.txt\n",
      "Generated ROUGE-2 report: results/evaluations_metrics/evaluation_mean_rouge2.txt\n",
      "Generated ROUGE-L report: results/evaluations_metrics/evaluation_mean_rougeL.txt\n",
      "Generated WER report: results/evaluations_metrics/evaluation_mean_wer.txt\n",
      "Generated Cosine Similarity report: results/evaluations_metrics/evaluation_mean_cosine.txt\n",
      "Generated Custom Evaluation report: results/evaluations_metrics/evaluation_mean_myevaluation.txt\n",
      "Generated Markdown table report: results/evaluations_metrics/table_report.md\n",
      "| Model - Instance | mean_cosine | mean_myevaluation | mean_wer | mean_rouge1 | mean_rouge2 | mean_rougeL |\n",
      "|---|---|---|---|---|---|---|\n",
      "| Seq2Seq3BiLSTM - Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4189 | 0.4189 | 1.1440 | 0.1564 | 0.0411 | 0.1552 |\n",
      "| Seq2Seq3BiLSTM - Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.4318 | 0.4318 | 1.1382 | 0.1665 | 0.0435 | 0.1653 |\n",
      "| Seq2Seq3BiLSTM - Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | 0.4198 | 0.4198 | 1.1611 | 0.1588 | 0.0383 | 0.1583 |\n",
      "| Seq2SeqBiLSTM - Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4631 | 0.4631 | 1.0647 | 0.2240 | 0.0696 | 0.2227 |\n",
      "| Seq2SeqBiLSTM - Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.4431 | 0.4431 | 1.0895 | 0.2075 | 0.0575 | 0.2052 |\n",
      "| Seq2SeqBiLSTM - Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | **0.4731** | **0.4731** | 1.0187 | **0.2416** | **0.0738** | **0.2397** |\n",
      "| Seq2SeqBiLSTMImproved - Seq2SeqBiLSTMImproved_optimizerAdam_lr0.001_ed300_ld256_do0.3_drdo0.3_edo0.3_erdo0.3_batch_size64_epochs50_summaries | 0.1034 | 0.1034 | **1.1710** | 0.0136 | 0.0000 | 0.0136 |\n",
      "| Seq2SeqGRU - Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4294 | 0.4294 | 1.0916 | 0.1656 | 0.0417 | 0.1650 |\n",
      "| Seq2SeqGRU - Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.3899 | 0.3899 | 1.1170 | 0.1290 | 0.0255 | 0.1287 |\n",
      "| Seq2SeqGRU - Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | 0.4445 | 0.4445 | 1.1173 | 0.1884 | 0.0563 | 0.1875 |\n",
      "| Seq2SeqLSTM - Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4400 | 0.4400 | 1.0826 | 0.1702 | 0.0453 | 0.1694 |\n",
      "| Seq2SeqLSTM - Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.4374 | 0.4374 | 1.0963 | 0.1629 | 0.0407 | 0.1616 |\n",
      "| Seq2SeqLSTM - Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | 0.4266 | 0.4266 | 1.1331 | 0.1607 | 0.0454 | 0.1604 |\n",
      "| Seq2SeqLSTMGlove - Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size128_epochs50_summaries | 0.4314 | 0.4314 | 1.0877 | 0.1798 | 0.0472 | 0.1795 |\n",
      "| Seq2SeqLSTMGlove - Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size256_epochs50_summaries | 0.4467 | 0.4467 | 1.0803 | 0.1927 | 0.0562 | 0.1918 |\n",
      "| Seq2SeqLSTMGlove - Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_batch_size64_epochs50_summaries | 0.4378 | 0.4378 | 1.1131 | 0.1861 | 0.0519 | 0.1855 |\n",
      "All reports generated.\n"
     ]
    }
   ],
   "source": [
    "TO_GENERATE_REPORT_SUMMARIES = True\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "from architectures.Seq2SeqBiLSTMImproved import Seq2SeqBiLSTMImproved\n",
    "model_classes = [\n",
    "    Seq2SeqGRU,\n",
    "    Seq2SeqLSTM,\n",
    "    Seq2SeqLSTMGlove,\n",
    "    Seq2SeqBiLSTM,\n",
    "    Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "    Seq2SeqBiLSTMImproved,\n",
    "]\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    evaluate_rouge,\n",
    "    evaluate_wer,\n",
    "    evaluate_cosine_similarity,\n",
    "    evaluate_myevalutation,\n",
    "    plot_rouge,\n",
    "    plot_wer,\n",
    "    plot_cosine_similarity,\n",
    "    plot_myevaluation,\n",
    ")\n",
    "import ast\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "# Aggiungi questa funzione utility\n",
    "def generate_metric_reports(all_metrics, results_root=\"results\"):\n",
    "    # Crea directory principale per i report\n",
    "    metrics_dir = os.path.join(results_root, \"evaluations_metrics\")\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "    # Definizione delle metriche e direzioni di ordinamento\n",
    "    metrics_config = {\n",
    "        \"mean_rouge1\": {\"name\": \"ROUGE-1\", \"ascending\": False},\n",
    "        \"mean_rouge2\": {\"name\": \"ROUGE-2\", \"ascending\": False},\n",
    "        \"mean_rougeL\": {\"name\": \"ROUGE-L\", \"ascending\": False},\n",
    "        \"mean_wer\": {\"name\": \"WER\", \"ascending\": True},\n",
    "        \"mean_cosine\": {\"name\": \"Cosine Similarity\", \"ascending\": False},\n",
    "        \"mean_myevaluation\": {\"name\": \"Custom Evaluation\", \"ascending\": False},\n",
    "    }\n",
    "\n",
    "    for metric, config in metrics_config.items():\n",
    "        # Ordina i risultati\n",
    "        sorted_metrics = sorted(\n",
    "            all_metrics, key=lambda x: x[metric], reverse=not config[\"ascending\"]\n",
    "        )\n",
    "\n",
    "        # Crea contenuto del report\n",
    "        report_content = f\"\"\"\n",
    "{'='*80}\n",
    "{config['name']} Metric Report - Sorted by {config['name']} ({'Descending' if not config['ascending'] else 'Ascending'})\n",
    "{'='*80}\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Total Models/Instances: {len(sorted_metrics)}\n",
    "\n",
    "{'='*80}\n",
    "{\"Rank\":<5} | {\"Model\":<20} | {\"Instance\":<40} | {config['name']:<10} | Other Metrics\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "\n",
    "        for rank, item in enumerate(sorted_metrics, 1):\n",
    "            other_metrics = \", \".join(\n",
    "                [\n",
    "                    f\"{k.split('_')[1].title()}: {v:.4f}\"\n",
    "                    for k, v in item.items()\n",
    "                    if k != \"model\" and k != \"instance\" and k != metric\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            report_content += (\n",
    "                f\"{rank:<5} | {item['model']:<20} | {item['instance']:<40} | \"\n",
    "                f\"{item[metric]:<10.4f} | {other_metrics}\\n\"\n",
    "            )\n",
    "\n",
    "        # Salva il file\n",
    "        report_path = os.path.join(metrics_dir, f\"evaluation_{metric}.txt\")\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(report_content)\n",
    "\n",
    "        print(f\"Generated {config['name']} report: {report_path}\")\n",
    "\n",
    "\n",
    "def generate_metric_reports_table(\n",
    "    all_metrics, output_path=\"results/evaluations_metrics/table_report.md\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera una tabella in formato Markdown che riassume le medie di ogni metrica per ogni istanza di modello.\n",
    "\n",
    "    - Le righe sono ordinate alfabeticamente in base al nome del modello e dell'istanza.\n",
    "    - Le colonne corrispondono alle seguenti metriche:\n",
    "        mean_rouge1, mean_rouge2, mean_rougeL, mean_wer, mean_cosine, mean_myevaluation\n",
    "    - In ogni cella viene visualizzata la media della metrica formattata a 4 decimali.\n",
    "    - Per ogni metrica, la cella con il valore massimo viene resa in grassetto.\n",
    "    - Il report viene salvato in output_path.\n",
    "    \"\"\"\n",
    "    # Definisci l'ordine delle metriche da visualizzare\n",
    "    metric_keys = [\n",
    "        \"mean_cosine\",\n",
    "        \"mean_myevaluation\",\n",
    "        \"mean_wer\",\n",
    "        \"mean_rouge1\",\n",
    "        \"mean_rouge2\",\n",
    "        \"mean_rougeL\",\n",
    "    ]\n",
    "\n",
    "    # Ordina le righe in base a \"model\" e \"instance\" in ordine alfabetico\n",
    "    sorted_rows = sorted(\n",
    "        all_metrics, key=lambda x: (x[\"model\"].lower(), x[\"instance\"].lower())\n",
    "    )\n",
    "\n",
    "    # Calcola il valore massimo per ogni metrica\n",
    "    max_per_metric = {key: max(row[key] for row in sorted_rows) for key in metric_keys}\n",
    "\n",
    "    # Costruisci l'intestazione della tabella Markdown\n",
    "    header = \"| Model - Instance | \" + \" | \".join(metric_keys) + \" |\"\n",
    "    separator = \"|---|\" + \"|\".join([\"---\"] * len(metric_keys)) + \"|\"\n",
    "\n",
    "    table_lines = [header, separator]\n",
    "\n",
    "    # Aggiungi ogni riga della tabella\n",
    "    for row in sorted_rows:\n",
    "        row_label = f\"{row['model']} - {row['instance']}\"\n",
    "        cell_values = []\n",
    "        for key in metric_keys:\n",
    "            value = row[key]\n",
    "            # Formatta il valore a 4 decimali\n",
    "            formatted_value = f\"{value:.4f}\"\n",
    "            # Se il valore è il massimo per quella metrica, rendilo in grassetto\n",
    "            if value == max_per_metric[key]:\n",
    "                formatted_value = f\"**{formatted_value}**\"\n",
    "            cell_values.append(formatted_value)\n",
    "        table_line = f\"| {row_label} | \" + \" | \".join(cell_values) + \" |\"\n",
    "        table_lines.append(table_line)\n",
    "\n",
    "    # Unisci le righe per ottenere il testo della tabella Markdown\n",
    "    table_md = \"\\n\".join(table_lines)\n",
    "\n",
    "    # Salva la tabella su file\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(table_md)\n",
    "\n",
    "    print(f\"Generated Markdown table report: {output_path}\")\n",
    "    print(table_md)\n",
    "\n",
    "\n",
    "\n",
    "model_instances = {}\n",
    "\n",
    "for model in model_classes:\n",
    "    model_name = str(model.__name__)\n",
    "    csv_dir = os.path.join(\"results\", model_name, \"csv\")\n",
    "\n",
    "    # Find all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "\n",
    "    # Filter only summaries files\n",
    "    summaries_files = [\n",
    "        f for f in csv_files if \"summaries_evaluated\" in os.path.basename(f).lower()\n",
    "    ]\n",
    "\n",
    "    # Extract file names\n",
    "    file_names = [os.path.basename(f) for f in summaries_files]\n",
    "\n",
    "    # Remove duplicates\n",
    "    file_names = list(set(file_names))\n",
    "\n",
    "    model_instances[model] = sorted(file_names)  # Order by name\n",
    "\n",
    "# Print model instances\n",
    "for model, instances in model_instances.items():\n",
    "    print(f\"Model: {model.__name__}\")\n",
    "    print(f\"CSV files found ({len(instances)}): {instances}\\n\")\n",
    "\n",
    "\n",
    "if TO_GENERATE_REPORT_SUMMARIES:\n",
    "    all_metrics = []\n",
    "\n",
    "    for model, instances in model_instances.items():\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Processing reports for {model.__name__}\")\n",
    "\n",
    "        for csv_file in instances:\n",
    "            # Costruisci il percorso completo\n",
    "            csv_path = os.path.join(\"results\", model.__name__, \"csv\", csv_file)\n",
    "\n",
    "            # Carica il CSV\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Funzione per estrarre i valori F1 da rouge_scores\n",
    "            import re\n",
    "\n",
    "            def parse_rouge(row):\n",
    "                try:\n",
    "                    text = row[\"rouge_scores\"]\n",
    "                    # Regex modificata per catturare sia numeri che 'L'\n",
    "                    pattern = r\"rouge([\\dL]+)': Score\\(precision=([\\d\\.]+), recall=([\\d\\.]+), fmeasure=([\\d\\.]+)\\)\"\n",
    "                    matches = re.findall(pattern, text)\n",
    "                    scores = {}\n",
    "                    for m in matches:\n",
    "                        key = f\"rouge{m[0]}\"  # Ora può essere 'rouge1', 'rouge2' o 'rougeL'\n",
    "                        scores[key] = float(m[3])\n",
    "                    # Assicurati che tutte le chiavi esistano\n",
    "                    for k in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                        scores.setdefault(k, 0.0)\n",
    "                    return scores\n",
    "                except Exception as e:\n",
    "                    return {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "\n",
    "            # Applica il parsing\n",
    "            rouge_values = df.apply(parse_rouge, axis=1)\n",
    "\n",
    "            # Calcola le medie\n",
    "            mean_rouge1 = np.mean([v[\"rouge1\"] for v in rouge_values])\n",
    "            mean_rouge2 = np.mean([v[\"rouge2\"] for v in rouge_values])\n",
    "            mean_rougeL = np.mean([v[\"rougeL\"] for v in rouge_values])\n",
    "            mean_wer = df[\"wer_scores\"].mean()\n",
    "            mean_cosine = df[\"cosine_similarity\"].mean()\n",
    "            mean_myevaluation = df[\"myevaluation_scores\"].mean()\n",
    "\n",
    "            # Crea il dizionario delle metriche\n",
    "            metrics_dict = {\n",
    "                \"mean_rouge1\": mean_rouge1,\n",
    "                \"mean_rouge2\": mean_rouge2,\n",
    "                \"mean_rougeL\": mean_rougeL,\n",
    "                \"mean_wer\": mean_wer,\n",
    "                \"mean_cosine\": mean_cosine,\n",
    "                \"mean_myevaluation\": mean_myevaluation,\n",
    "                \"model\": model.__name__,\n",
    "                \"instance\": os.path.splitext(csv_file)[0].replace(\"_evaluated\", \"\"),\n",
    "            }\n",
    "\n",
    "            all_metrics.append(metrics_dict)\n",
    "            print(f\"Processed: {csv_file}\")\n",
    "\n",
    "    # Genera i report aggregati per metrica\n",
    "    generate_metric_reports(all_metrics)\n",
    "    generate_metric_reports_table(all_metrics)\n",
    "    print(\"All reports generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
