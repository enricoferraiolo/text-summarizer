{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script assurdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello Ã¨ Seq2Seq con attention, utilizzando LSTM come encoder e decoder. Inoltre, il modello utilizza un layer di attenzione (customizzato con AttentionLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 08:21:30.690878: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-26 08:21:30.699872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740554490.710013    8386 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740554490.712791    8386 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 08:21:30.723685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 2\n",
      "\n",
      "==================================================\n",
      "Training: Seq2SeqLSTMGlove\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings already downloaded.\n",
      "Extracting ./architectures/weightsGLOVE/glove.6B.zip...\n",
      "Extraction complete.\n",
      "GloVe embeddings already downloaded.\n",
      "Extracting ./architectures/weightsGLOVE/glove.6B.zip...\n",
      "Extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740554521.600524    8386 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5807 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqLSTMGlove' object has no attribute 'encoder_dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 207\u001b[0m\n\u001b[1;32m    193\u001b[0m (\n\u001b[1;32m    194\u001b[0m     x_voc,\n\u001b[1;32m    195\u001b[0m     y_voc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m     max_summary_len,\n\u001b[1;32m    204\u001b[0m ) \u001b[38;5;241m=\u001b[39m prepare_data()\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Create the model instance\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_voc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_voc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_voc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_voc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_text_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_text_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_summary_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_summary_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname_additional_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_optimizer\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_lr\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ed\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ld\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatent_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_do\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_dropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_drdo\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_recurrent_dropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_edo\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_dropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_erdo\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_recurrent_dropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatent_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_recurrent_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_recurrent_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoder_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_recurrent_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoder_recurrent_dropout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Plot the model architecture\u001b[39;00m\n\u001b[1;32m    224\u001b[0m TO_SAVE_MODEL_ARCHITECTURE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/mega-training/architectures/Seq2SeqLSTMGlove.py:108\u001b[0m, in \u001b[0;36mSeq2SeqLSTMGlove.__init__\u001b[0;34m(self, x_voc, y_voc, max_text_len, max_summary_len, x_tokenizer, y_tokenizer, name, name_additional_info, latent_dim, embedding_dim, encoder_dropout, encoder_recurrent_dropout, decoder_dropout, decoder_recurrent_dropout, glove_dim)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_embedding \u001b[38;5;241m=\u001b[39m Embedding(\n\u001b[1;32m     94\u001b[0m     x_voc,\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_embedding \u001b[38;5;241m=\u001b[39m Embedding(\n\u001b[1;32m    101\u001b[0m     y_voc,\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_lstm1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_lstm2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_lstm3 \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_encoder_lstm_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_lstm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_decoder_lstm_layer()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layer \u001b[38;5;241m=\u001b[39m AttentionLayer(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/text-summarizer/mega-training/architectures/Seq2SeqLSTMGlove.py:165\u001b[0m, in \u001b[0;36mSeq2SeqLSTMGlove.get_encoder_lstm_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_encoder_lstm_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    161\u001b[0m         LSTM(\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim,\n\u001b[1;32m    163\u001b[0m             return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    164\u001b[0m             return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m--> 165\u001b[0m             dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_dropout\u001b[49m,\n\u001b[1;32m    166\u001b[0m             recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_recurrent_dropout,\n\u001b[1;32m    167\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_lstm1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    168\u001b[0m         ),\n\u001b[1;32m    169\u001b[0m         LSTM(\n\u001b[1;32m    170\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim,\n\u001b[1;32m    171\u001b[0m             return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    172\u001b[0m             return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    173\u001b[0m             dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_dropout,\n\u001b[1;32m    174\u001b[0m             recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_recurrent_dropout,\n\u001b[1;32m    175\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_lstm2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m         ),\n\u001b[1;32m    177\u001b[0m         LSTM(\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim,\n\u001b[1;32m    179\u001b[0m             return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    180\u001b[0m             return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    181\u001b[0m             dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_dropout,\n\u001b[1;32m    182\u001b[0m             recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_recurrent_dropout,\n\u001b[1;32m    183\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_lstm3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    184\u001b[0m         ),\n\u001b[1;32m    185\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Seq2SeqLSTMGlove' object has no attribute 'encoder_dropout'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    evaluate_rouge,\n",
    "    evaluate_wer,\n",
    "    evaluate_cosine_similarity,\n",
    "    plot_rouge,\n",
    "    plot_wer,\n",
    "    plot_cosine_similarity,\n",
    "    generate_summaries,\n",
    "    create_hyperparameter_grid,\n",
    "    prepare_data,\n",
    ")\n",
    "\n",
    "\n",
    "def save_metrics_results(df_summaries, model_name, results_path):\n",
    "    metrics_file_path = f\"{results_path}/csv/{model_name}_metrics_scores.csv\"\n",
    "    df_summaries.to_csv(metrics_file_path, index=False)\n",
    "    print(f\"Metrics results saved to {metrics_file_path}\")\n",
    "\n",
    "\n",
    "def plot_training_history(history, model_name, save_path):\n",
    "    plt.plot(history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Model Loss Over Epochs - {model_name}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(\n",
    "        os.path.join(save_path, f\"{model_name}_lossplot.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_model(model, model_name, save_path, save_full_model=True):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # Save the model weights\n",
    "    # model.save_weights(os.path.join(save_path, f\"{model_name}.weights.h5\"))\n",
    "    if save_full_model:\n",
    "        # Save the full model\n",
    "        model.save(os.path.join(save_path, f\"{model_name}_full_model.h5\"))\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_instance,\n",
    "    hyperparams,\n",
    "    x_training_padded,\n",
    "    y_training_padded,\n",
    "    x_validation_padded,\n",
    "    y_validation_padded,\n",
    "    save_path,\n",
    "):\n",
    "    K.clear_session()\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    latent_dim = hyperparams[\"latent_dim\"]\n",
    "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
    "    encoder_dropout = hyperparams[\"encoder_dropout\"]\n",
    "    encoder_recurrent_dropout = hyperparams[\"encoder_recurrent_dropout\"]\n",
    "    decoder_dropout = hyperparams[\"decoder_dropout\"]\n",
    "    decoder_recurrent_dropout = hyperparams[\"decoder_recurrent_dropout\"]\n",
    "    optimizer_class = hyperparams[\"optimizer_class\"]\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    learning_rate = hyperparams[\"learning_rate\"]\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "\n",
    "    # Set optimizer and callbacks\n",
    "    model_instance.change_optimizer(optimizer)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    def lr_schedule(epoch, lr):\n",
    "        decay_rate = 0.95\n",
    "        decay_step = 1\n",
    "        if epoch % decay_step == 0 and epoch != 0:\n",
    "            return lr * decay_rate\n",
    "        return lr\n",
    "\n",
    "    learning_rate_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "    # Reduce LR on Plateau\n",
    "    reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    # Add callbacks to the model instance\n",
    "    model_instance.add_callbacks(\n",
    "        [early_stopping, learning_rate_scheduler, reduce_lr_on_plateau]\n",
    "    )\n",
    "\n",
    "    model = model_instance.get_model()\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        [x_training_padded, y_training_padded[:, :-1]],\n",
    "        y_training_padded.reshape(\n",
    "            y_training_padded.shape[0], y_training_padded.shape[1], 1\n",
    "        )[:, 1:],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(\n",
    "            [x_validation_padded, y_validation_padded[:, :-1]],\n",
    "            y_validation_padded.reshape(\n",
    "                y_validation_padded.shape[0], y_validation_padded.shape[1], 1\n",
    "            )[:, 1:],\n",
    "        ),\n",
    "        callbacks=model_instance.get_callbacks(),\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    TO_SAVE_MODEL = False\n",
    "    model_name = model_instance.name\n",
    "    model_save_path = os.path.join(save_path, \"weights\")\n",
    "    if TO_SAVE_MODEL:\n",
    "        save_model(model, model_name, model_save_path)\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        history.history, model_name, os.path.join(save_path, \"graphs\")\n",
    "    )\n",
    "\n",
    "    return history.history\n",
    "\n",
    "\n",
    "# Define hyperparameter grid\n",
    "hyperparameter_grid = create_hyperparameter_grid()\n",
    "\n",
    "# Define models\n",
    "model_classes = [\n",
    "    #Seq2SeqGRU,\n",
    "    #Seq2SeqLSTM,\n",
    "    Seq2SeqLSTMGlove,\n",
    "    #Seq2SeqBiLSTM,\n",
    "    #Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "]\n",
    "\n",
    "# Training loop\n",
    "results_path = f\"results/\"\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "for model_class in model_classes:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Training: {model_class.__name__}\")\n",
    "\n",
    "    results_path = f\"results/{model_class.__name__}\"\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    # Crea the subdirectories\n",
    "    os.makedirs(f\"{results_path}/weights\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/graphs\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/architectures\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/csv\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/histories\", exist_ok=True)\n",
    "\n",
    "    for hyperparams in hyperparameter_grid:\n",
    "        # Get prepared data\n",
    "        (\n",
    "            x_voc,\n",
    "            y_voc,\n",
    "            x_tokenizer,\n",
    "            y_tokenizer,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            max_text_len,\n",
    "            max_summary_len,\n",
    "        ) = prepare_data()\n",
    "\n",
    "        # Create the model instance\n",
    "        model_instance = model_class(\n",
    "            x_voc=x_voc,\n",
    "            y_voc=y_voc,\n",
    "            max_text_len=max_text_len,\n",
    "            max_summary_len=max_summary_len,\n",
    "            x_tokenizer=x_tokenizer,\n",
    "            y_tokenizer=y_tokenizer,\n",
    "            name_additional_info=f\"_optimizer{hyperparams['optimizer_class'].__name__}_lr{hyperparams['learning_rate']}_ed{hyperparams['embedding_dim']}_ld{hyperparams['latent_dim']}_do{hyperparams['decoder_dropout']}_drdo{hyperparams['decoder_recurrent_dropout']}_edo{hyperparams['encoder_dropout']}_erdo{hyperparams['encoder_recurrent_dropout']}\",\n",
    "            latent_dim=hyperparams[\"latent_dim\"],\n",
    "            embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "            encoder_dropout=hyperparams[\"encoder_dropout\"],\n",
    "            encoder_recurrent_dropout=hyperparams[\"encoder_recurrent_dropout\"],\n",
    "            decoder_dropout=hyperparams[\"decoder_dropout\"],\n",
    "            decoder_recurrent_dropout=hyperparams[\"decoder_recurrent_dropout\"],\n",
    "        )\n",
    "\n",
    "        # Plot the model architecture\n",
    "        TO_SAVE_MODEL_ARCHITECTURE = False\n",
    "        if TO_SAVE_MODEL_ARCHITECTURE:\n",
    "            plot_model(\n",
    "                model_instance.get_model(),\n",
    "                to_file=f\"{results_path}/media/architectures/{model_instance.name}_architecture.png\",\n",
    "                show_shapes=True,\n",
    "            )\n",
    "\n",
    "        print(f\"Training {model_instance.name} with hyperparameters {hyperparams}\")\n",
    "        history = train_model(\n",
    "            model_instance,\n",
    "            hyperparams,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            results_path,\n",
    "        )\n",
    "\n",
    "        # Save training history\n",
    "        history_path = os.path.join(\n",
    "            results_path, f\"histories/{model_instance.name}_history.txt\"\n",
    "        )\n",
    "        with open(history_path, \"a\") as f:\n",
    "            f.write(f\"Hyperparameters: {hyperparams}\\n\")\n",
    "            f.write(f\"History: {history}\\n\\n\")\n",
    "            # Write last epoch loss, val_loss, accuracy, val_accuracy\n",
    "            f.write(\n",
    "                f\"Last epoch loss: {history['loss'][-1]}, val_loss: {history['val_loss'][-1]}\\n\"\n",
    "            )\n",
    "\n",
    "        # Generate and save summaries\n",
    "        print(f\"Generating summaries for {model_instance.name}\")\n",
    "        summaries_path = os.path.join(results_path, \"csv\")\n",
    "        df_summaries = generate_summaries(\n",
    "            model_instance,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            max_text_len,\n",
    "            n_summaries=1000,\n",
    "            save_path=summaries_path,\n",
    "        )\n",
    "\n",
    "        df_summaries, mean_scores_rouge = evaluate_rouge(df_summaries)\n",
    "        df_summaries, mean_score_wer = evaluate_wer(df_summaries)\n",
    "        df_summaries, mean_score_cosine_similarity = evaluate_cosine_similarity(\n",
    "            df_summaries\n",
    "        )\n",
    "\n",
    "        # Save evaluation results\n",
    "        TO_SAVE_METRICS_RESULTS = True\n",
    "        if TO_SAVE_METRICS_RESULTS:\n",
    "            save_metrics_results(df_summaries, model_instance.name, results_path)\n",
    "\n",
    "        # Print mean scores in history file\n",
    "        with open(history_path, \"a\") as f:\n",
    "            f.write(f\"Mean ROUGE scores: {mean_scores_rouge}\\n\")\n",
    "            f.write(f\"Mean WER score: {mean_score_wer}\\n\")\n",
    "            f.write(f\"Mean Cosine Similarity score: {mean_score_cosine_similarity}\\n\\n\")\n",
    "\n",
    "        # Plot evaluation results\n",
    "        TO_SAVE_PLOTS = False\n",
    "\n",
    "        if TO_SAVE_PLOTS:\n",
    "            plot_rouge(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                metric=\"rouge1\",\n",
    "                title=f\"ROUGE-1 Score Distribution - {model_instance.name}\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "\n",
    "            plot_rouge(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                metric=\"rouge2\",\n",
    "                title=f\"ROUGE-2 Score Distribution - {model_instance.name}\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "            plot_rouge(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                metric=\"rougeL\",\n",
    "                title=f\"ROUGE-L Score Distribution - {model_instance.name}\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "            plot_wer(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                title=f\"WER Score Distribution - {model_instance.name}\",\n",
    "                color=\"red\",\n",
    "            )\n",
    "            plot_cosine_similarity(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                title=f\"Cosine Similarity Distribution - {model_instance.name}\",\n",
    "                color=\"green\",\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
