{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script assurdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello Ã¨ Seq2Seq con attention, utilizzando LSTM come encoder e decoder. Inoltre, il modello utilizza un layer di attenzione (customizzato con AttentionLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 1\n",
      "\n",
      "==================================================\n",
      "Training: Seq2SeqLSTMGlove\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_680360/2381037303.py\", line 206, in <module>\n",
      "    ) = prepare_data()\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/mega-training/utils.py\", line 601, in prepare_data\n",
      "  File \"/home/enrico/Desktop/text-summarizer/mega-training/utils.py\", line 580, in clean_text\n",
      "    for word in cleaned_text.split()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/re/__init__.py\", line 186, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    evaluate_rouge,\n",
    "    evaluate_wer,\n",
    "    evaluate_cosine_similarity,\n",
    "    evaluate_myevalutation,\n",
    "    plot_rouge,\n",
    "    plot_wer,\n",
    "    plot_cosine_similarity,\n",
    "    generate_summaries,\n",
    "    create_hyperparameter_grid,\n",
    "    prepare_data,\n",
    "    plot_myevaluation,\n",
    ")\n",
    "\n",
    "\n",
    "def save_metrics_results(df_summaries, model_name, results_path):\n",
    "    metrics_file_path = f\"{results_path}/csv/{model_name}_metrics_scores.csv\"\n",
    "    df_summaries.to_csv(metrics_file_path, index=False)\n",
    "    print(f\"Metrics results saved to {metrics_file_path}\")\n",
    "\n",
    "\n",
    "def plot_training_history(history, model_name, save_path):\n",
    "    plt.plot(history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Model Loss Over Epochs - {model_name}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(\n",
    "        os.path.join(save_path, f\"{model_name}_lossplot.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_model(model, model_name, save_path, save_full_model=True):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # Save the model weights\n",
    "    # model.save_weights(os.path.join(save_path, f\"{model_name}.weights.h5\"))\n",
    "    if save_full_model:\n",
    "        # Save the full model\n",
    "        model.save(os.path.join(save_path, f\"{model_name}_full_model.h5\"))\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_instance,\n",
    "    hyperparams,\n",
    "    x_training_padded,\n",
    "    y_training_padded,\n",
    "    x_validation_padded,\n",
    "    y_validation_padded,\n",
    "    save_path,\n",
    "):\n",
    "    K.clear_session()\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    latent_dim = hyperparams[\"latent_dim\"]\n",
    "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
    "    encoder_dropout = hyperparams[\"encoder_dropout\"]\n",
    "    encoder_recurrent_dropout = hyperparams[\"encoder_recurrent_dropout\"]\n",
    "    decoder_dropout = hyperparams[\"decoder_dropout\"]\n",
    "    decoder_recurrent_dropout = hyperparams[\"decoder_recurrent_dropout\"]\n",
    "    optimizer_class = hyperparams[\"optimizer_class\"]\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    learning_rate = hyperparams[\"learning_rate\"]\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "\n",
    "    # Set optimizer and callbacks\n",
    "    model_instance.change_optimizer(optimizer)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    def lr_schedule(epoch, lr):\n",
    "        decay_rate = 0.95\n",
    "        decay_step = 1\n",
    "        if epoch % decay_step == 0 and epoch != 0:\n",
    "            return lr * decay_rate\n",
    "        return lr\n",
    "\n",
    "    learning_rate_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "    # Reduce LR on Plateau\n",
    "    reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    # Add callbacks to the model instance\n",
    "    model_instance.add_callbacks(\n",
    "        [early_stopping, learning_rate_scheduler, reduce_lr_on_plateau]\n",
    "    )\n",
    "\n",
    "    model = model_instance.get_model()\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        [x_training_padded, y_training_padded[:, :-1]],\n",
    "        y_training_padded.reshape(\n",
    "            y_training_padded.shape[0], y_training_padded.shape[1], 1\n",
    "        )[:, 1:],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(\n",
    "            [x_validation_padded, y_validation_padded[:, :-1]],\n",
    "            y_validation_padded.reshape(\n",
    "                y_validation_padded.shape[0], y_validation_padded.shape[1], 1\n",
    "            )[:, 1:],\n",
    "        ),\n",
    "        callbacks=model_instance.get_callbacks(),\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    TO_SAVE_MODEL = False\n",
    "    model_name = model_instance.name\n",
    "    model_save_path = os.path.join(save_path, \"weights\")\n",
    "    if TO_SAVE_MODEL:\n",
    "        save_model(model, model_name, model_save_path)\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        history.history, model_name, os.path.join(save_path, \"media/graphs\")\n",
    "    )\n",
    "\n",
    "    return history.history\n",
    "\n",
    "\n",
    "# Define hyperparameter grid\n",
    "hyperparameter_grid = create_hyperparameter_grid()\n",
    "\n",
    "# Define models\n",
    "model_classes = [\n",
    "    #Seq2SeqGRU,\n",
    "    #Seq2SeqLSTM,\n",
    "    Seq2SeqLSTMGlove,\n",
    "    Seq2SeqBiLSTM,\n",
    "    Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "]\n",
    "\n",
    "# Training loop\n",
    "results_path = f\"results/\"\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "for model_class in model_classes:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Training: {model_class.__name__}\")\n",
    "\n",
    "    results_path = f\"results/{model_class.__name__}\"\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    # Crea the subdirectories\n",
    "    os.makedirs(f\"{results_path}/weights\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/graphs\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/architectures\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/csv\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/histories\", exist_ok=True)\n",
    "\n",
    "    for hyperparams in hyperparameter_grid:\n",
    "        # Get prepared data\n",
    "        (\n",
    "            x_voc,\n",
    "            y_voc,\n",
    "            x_tokenizer,\n",
    "            y_tokenizer,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            max_text_len,\n",
    "            max_summary_len,\n",
    "        ) = prepare_data()\n",
    "\n",
    "        # Create the model instance\n",
    "        model_instance = model_class(\n",
    "            x_voc=x_voc,\n",
    "            y_voc=y_voc,\n",
    "            max_text_len=max_text_len,\n",
    "            max_summary_len=max_summary_len,\n",
    "            x_tokenizer=x_tokenizer,\n",
    "            y_tokenizer=y_tokenizer,\n",
    "            name_additional_info=f\"_optimizer{hyperparams['optimizer_class'].__name__}_lr{hyperparams['learning_rate']}_ed{hyperparams['embedding_dim']}_ld{hyperparams['latent_dim']}_do{hyperparams['decoder_dropout']}_drdo{hyperparams['decoder_recurrent_dropout']}_edo{hyperparams['encoder_dropout']}_erdo{hyperparams['encoder_recurrent_dropout']}\",\n",
    "            latent_dim=hyperparams[\"latent_dim\"],\n",
    "            embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "            encoder_dropout=hyperparams[\"encoder_dropout\"],\n",
    "            encoder_recurrent_dropout=hyperparams[\"encoder_recurrent_dropout\"],\n",
    "            decoder_dropout=hyperparams[\"decoder_dropout\"],\n",
    "            decoder_recurrent_dropout=hyperparams[\"decoder_recurrent_dropout\"],\n",
    "        )\n",
    "\n",
    "        # Plot the model architecture\n",
    "        TO_SAVE_MODEL_ARCHITECTURE = False\n",
    "        if TO_SAVE_MODEL_ARCHITECTURE:\n",
    "            plot_model(\n",
    "                model_instance.get_model(),\n",
    "                to_file=f\"{results_path}/media/architectures/{model_instance.name}_architecture.png\",\n",
    "                show_shapes=True,\n",
    "            )\n",
    "\n",
    "        print(f\"Training {model_instance.name} with hyperparameters {hyperparams}\")\n",
    "        history = train_model(\n",
    "            model_instance,\n",
    "            hyperparams,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            results_path,\n",
    "        )\n",
    "\n",
    "        # Save training history\n",
    "        history_path = os.path.join(\n",
    "            results_path, f\"histories/{model_instance.name}_history.txt\"\n",
    "        )\n",
    "        with open(history_path, \"a\") as f:\n",
    "            f.write(f\"Hyperparameters: {hyperparams}\\n\")\n",
    "            f.write(f\"History: {history}\\n\\n\")\n",
    "            # Write last epoch loss, val_loss, accuracy, val_accuracy\n",
    "            f.write(\n",
    "                f\"Last epoch loss: {history['loss'][-1]}, val_loss: {history['val_loss'][-1]}\\n\"\n",
    "            )\n",
    "\n",
    "        TO_GENERATE_SUMMARIES = True\n",
    "        if TO_GENERATE_SUMMARIES:\n",
    "            # Generate and save summaries\n",
    "            print(f\"Generating summaries for {model_instance.name}\")\n",
    "            summaries_path = os.path.join(results_path, \"csv\")\n",
    "            df_summaries = generate_summaries(\n",
    "                model_instance,\n",
    "                x_training_padded,\n",
    "                y_training_padded,\n",
    "                max_text_len,\n",
    "                n_summaries=1000,\n",
    "                save_path=summaries_path,\n",
    "            )\n",
    "\n",
    "            TO_EVALUATE_SUMMARIES = False\n",
    "            if TO_EVALUATE_SUMMARIES:\n",
    "                # Evaluate summaries\n",
    "                df_summaries, mean_scores_rouge = evaluate_rouge(df_summaries)\n",
    "                df_summaries, mean_score_wer = evaluate_wer(df_summaries)\n",
    "                df_summaries, mean_score_cosine_similarity = evaluate_cosine_similarity(\n",
    "                    df_summaries\n",
    "                )\n",
    "                df_summaries, mean_score_myevaluation = evaluate_myevalutation(df_summaries)\n",
    "\n",
    "                # Save evaluation results\n",
    "                TO_SAVE_METRICS_RESULTS = True\n",
    "                if TO_SAVE_METRICS_RESULTS:\n",
    "                    save_metrics_results(df_summaries, model_instance.name, results_path)\n",
    "\n",
    "                # Print mean scores in history file\n",
    "                with open(history_path, \"a\") as f:\n",
    "                    f.write(f\"Mean ROUGE scores: {mean_scores_rouge}\\n\")\n",
    "                    f.write(f\"Mean WER score: {mean_score_wer}\\n\")\n",
    "                    f.write(f\"Mean Cosine Similarity score: {mean_score_cosine_similarity}\\n\\n\")\n",
    "                    f.write(f\"Mean My Evaluation score: {mean_score_myevaluation}\\n\\n\")\n",
    "\n",
    "                # Plot evaluation results\n",
    "                TO_SAVE_PLOTS = True\n",
    "                if TO_SAVE_PLOTS:\n",
    "                    plot_rouge(\n",
    "                        df_summaries,\n",
    "                        f\"{results_path}/media/graphs\",\n",
    "                        model_instance.name,\n",
    "                        metric=\"rouge1\",\n",
    "                        title=f\"ROUGE-1 Score Distribution - {model_instance.name}\",\n",
    "                        color=\"blue\",\n",
    "                    )\n",
    "\n",
    "                    plot_rouge(\n",
    "                        df_summaries,\n",
    "                        f\"{results_path}/media/graphs\",\n",
    "                        model_instance.name,\n",
    "                        metric=\"rouge2\",\n",
    "                        title=f\"ROUGE-2 Score Distribution - {model_instance.name}\",\n",
    "                        color=\"blue\",\n",
    "                    )\n",
    "                    plot_rouge(\n",
    "                        df_summaries,\n",
    "                        f\"{results_path}/media/graphs\",\n",
    "                        model_instance.name,\n",
    "                        metric=\"rougeL\",\n",
    "                        title=f\"ROUGE-L Score Distribution - {model_instance.name}\",\n",
    "                        color=\"blue\",\n",
    "                    )\n",
    "                    plot_wer(\n",
    "                        df_summaries,\n",
    "                        f\"{results_path}/media/graphs\",\n",
    "                        model_instance.name,\n",
    "                        title=f\"WER Score Distribution - {model_instance.name}\",\n",
    "                        color=\"red\",\n",
    "                    )\n",
    "                    plot_cosine_similarity(\n",
    "                        df_summaries,\n",
    "                        f\"{results_path}/media/graphs\",\n",
    "                        model_instance.name,\n",
    "                        title=f\"Cosine Similarity Distribution - {model_instance.name}\",\n",
    "                        color=\"green\",\n",
    "                    )\n",
    "                    plot_myevaluation(\n",
    "                        df_summaries,\n",
    "                        f\"{results_path}/media/graphs\",\n",
    "                        model_instance.name,\n",
    "                        title=f\"My Evaluation Distribution - {model_instance.name}\",\n",
    "                        color=\"purple\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 12:11:11.794920: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-27 12:11:11.803213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740654671.812641  682225 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740654671.815355  682225 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-27 12:11:11.826360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2SeqGRU\n",
      "CSV files found (2): ['Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv', 'Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries_evaluated.csv']\n",
      "\n",
      "Model: Seq2SeqLSTM\n",
      "CSV files found (1): ['Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqLSTMGlove\n",
      "CSV files found (1): ['Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv']\n",
      "\n",
      "Model: Seq2SeqBiLSTM\n",
      "CSV files found (1): ['Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv']\n",
      "\n",
      "Model: Seq2Seq3BiLSTM\n",
      "CSV files found (1): ['Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv']\n",
      "\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqGRU\n",
      "Evaluating file: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico/Desktop/text-summarizer/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries_evaluated.csv\n",
      "Evaluating file: Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries_evaluated.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqGRU/csv/Seq2SeqGRU_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries_evaluated_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqLSTM\n",
      "Evaluating file: Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTM/csv/Seq2SeqLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqLSTMGlove\n",
      "Evaluating file: Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqLSTMGlove/csv/Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2SeqBiLSTM\n",
      "Evaluating file: Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2SeqBiLSTM/csv/Seq2SeqBiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries_evaluated.csv\n",
      "==================================================\n",
      "Evaluating summaries for Seq2Seq3BiLSTM\n",
      "Evaluating file: Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries.csv\n",
      "Evaluating rouge\n",
      "Evaluating wer\n",
      "Evaluating cosine similarity\n",
      "Evaluating my evaluation\n",
      "Finished evaluation\n",
      "Evaluated file: results/Seq2Seq3BiLSTM/csv/Seq2Seq3BiLSTM_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2_summaries_evaluated.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    evaluate_rouge,\n",
    "    evaluate_wer,\n",
    "    evaluate_cosine_similarity,\n",
    "    evaluate_myevalutation,\n",
    "    plot_rouge,\n",
    "    plot_wer,\n",
    "    plot_cosine_similarity,\n",
    "    plot_myevaluation,\n",
    ")\n",
    "import glob\n",
    "\n",
    "model_classes = [\n",
    "    Seq2SeqGRU,\n",
    "    Seq2SeqLSTM,\n",
    "    Seq2SeqLSTMGlove,\n",
    "    Seq2SeqBiLSTM,\n",
    "    Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "]\n",
    "\n",
    "model_instances = {}\n",
    "\n",
    "for model in model_classes:\n",
    "    model_name = str(model.__name__)\n",
    "    csv_dir = os.path.join(\"results\", model_name, \"csv\")\n",
    "\n",
    "    # Find all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "\n",
    "    # Filter only summaries files\n",
    "    summaries_files = [\n",
    "        f for f in csv_files if \"summaries\" in os.path.basename(f).lower()\n",
    "    ]\n",
    "\n",
    "    # Extract file names\n",
    "    file_names = [os.path.basename(f) for f in summaries_files]\n",
    "\n",
    "    # Remove duplicates\n",
    "    file_names = list(set(file_names))\n",
    "\n",
    "    model_instances[model] = sorted(file_names)  # Order by name\n",
    "\n",
    "# Print model instances\n",
    "for model, instances in model_instances.items():\n",
    "    print(f\"Model: {model.__name__}\")\n",
    "    print(f\"CSV files found ({len(instances)}): {instances}\\n\")\n",
    "\n",
    "\n",
    "def save_metrics_results(df_summaries, model_name, results_path):\n",
    "    metrics_file_path = f\"{results_path}/csv/{model_name}_metrics_scores.csv\"\n",
    "    df_summaries.to_csv(metrics_file_path, index=False)\n",
    "    print(f\"Metrics results saved to {metrics_file_path}\")\n",
    "\n",
    "\n",
    "TO_EVALUATE_SUMMARIES = True\n",
    "if TO_EVALUATE_SUMMARIES:\n",
    "    # Iterate through all models and their instances\n",
    "    for model, instances in model_instances.items():\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Evaluating summaries for {model.__name__}\")\n",
    "        for csv_file in instances:\n",
    "            print(f\"Evaluating file: {csv_file}\")\n",
    "\n",
    "            # Load original csv\n",
    "            original_path = os.path.join(\"results\", model.__name__, \"csv\", csv_file)\n",
    "            df_summaries = pd.read_csv(original_path)\n",
    "\n",
    "            # Evaluate summaries\n",
    "            print(f\"Evaluating rouge\")\n",
    "            df_summaries, mean_scores_rouge = evaluate_rouge(df_summaries)\n",
    "            print(f\"Evaluating wer\")\n",
    "            df_summaries, mean_score_wer = evaluate_wer(df_summaries)\n",
    "            print(f\"Evaluating cosine similarity\")\n",
    "            df_summaries, mean_score_cosine_similarity = evaluate_cosine_similarity(\n",
    "                df_summaries\n",
    "            )\n",
    "            print(f\"Evaluating my evaluation\")\n",
    "            df_summaries, mean_score_myevaluation = evaluate_myevalutation(df_summaries)\n",
    "\n",
    "            print(\"Finished evaluation\")\n",
    "\n",
    "            # Create new file name\n",
    "            base_name = os.path.splitext(csv_file)[0]\n",
    "            evaluated_filename = f\"{base_name}_evaluated.csv\"\n",
    "            evaluated_path = os.path.join(\n",
    "                \"results\", model.__name__, \"csv\", evaluated_filename\n",
    "            )\n",
    "\n",
    "            # Save evaluated file\n",
    "            df_summaries.to_csv(evaluated_path, index=False)\n",
    "            print(f\"Evaluated file: {evaluated_path}\")\n",
    "\n",
    "\n",
    "            results_path = f\"results/{model.__name__}\"\n",
    "            \n",
    "            # Plotting\n",
    "            TO_SAVE_PLOTS = True\n",
    "            if TO_SAVE_PLOTS:\n",
    "                graph_dir = os.path.join(results_path, \"media/graphs\", base_name)\n",
    "                os.makedirs(graph_dir, exist_ok=True)\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rouge1\",\n",
    "                    title=f\"ROUGE-1 - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rouge2\",\n",
    "                    title=f\"ROUGE-2 - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_rouge(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    metric=\"rougeL\",\n",
    "                    title=f\"ROUGE-L - {base_name}\",\n",
    "                    color=\"blue\",\n",
    "                )\n",
    "\n",
    "                plot_wer(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"WER - {base_name}\",\n",
    "                    color=\"red\",\n",
    "                )\n",
    "\n",
    "                plot_cosine_similarity(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"Cosine Similarity - {base_name}\",\n",
    "                    color=\"green\",\n",
    "                )\n",
    "\n",
    "                plot_myevaluation(\n",
    "                    df_summaries,\n",
    "                    graph_dir,\n",
    "                    base_name,\n",
    "                    title=f\"My Evaluation - {base_name}\",\n",
    "                    color=\"purple\",\n",
    "                )\n",
    "\n",
    "            # Update history file\n",
    "            history_path = os.path.join(\n",
    "                results_path, f\"histories/{base_name}_history\"\n",
    "            )\n",
    "            with open(history_path, \"a\") as f:\n",
    "                f.write(f\"\\nEvaluation for {csv_file}:\\n\")\n",
    "                f.write(f\"Mean ROUGE scores: {mean_scores_rouge}\\n\")\n",
    "                f.write(f\"Mean WER score: {mean_score_wer}\\n\")\n",
    "                f.write(f\"Mean Cosine Similarity: {mean_score_cosine_similarity}\\n\")\n",
    "                f.write(f\"Mean My Evaluation: {mean_score_myevaluation}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
