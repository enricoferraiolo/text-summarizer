{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script assurdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello è Seq2Seq con attention, utilizzando LSTM come encoder e decoder. Inoltre, il modello utilizza un layer di attenzione (customizzato con AttentionLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 2\n",
      "\n",
      "==================================================\n",
      "Training: Seq2SeqLSTMGlove\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings already downloaded.\n",
      "Extracting ./architectures/weightsGLOVE/glove.6B.zip...\n",
      "Extraction complete.\n",
      "GloVe embeddings already downloaded.\n",
      "Extracting ./architectures/weightsGLOVE/glove.6B.zip...\n",
      "Extraction complete.\n",
      "Training Seq2SeqLSTMGlove_optimizerAdam_lr0.001_ed512_ld256_do0.2_drdo0.2_edo0.2_erdo0.2 with hyperparameters {'latent_dim': 256, 'embedding_dim': 512, 'encoder_dropout': 0.2, 'encoder_recurrent_dropout': 0.2, 'decoder_dropout': 0.2, 'decoder_recurrent_dropout': 0.2, 'optimizer_class': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'epochs': 50, 'batch_size': 128}\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/50\n",
      "\u001b[1m323/323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.5283 - loss: 3.3231 - val_accuracy: 0.6032 - val_loss: 2.5272 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
      "Epoch 2/50\n",
      "\u001b[1m323/323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.6013 - loss: 2.5242 - val_accuracy: 0.6144 - val_loss: 2.3757 - learning_rate: 9.5000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
      "Epoch 3/50\n",
      "\u001b[1m254/323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.6107 - loss: 2.3832"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "import pandas as pd\n",
    "\n",
    "from utils import (\n",
    "    evaluate_rouge,\n",
    "    evaluate_wer,\n",
    "    evaluate_cosine_similarity,\n",
    "    plot_rouge,\n",
    "    plot_wer,\n",
    "    plot_cosine_similarity,\n",
    "    generate_summaries,\n",
    "    create_hyperparameter_grid,\n",
    "    prepare_data,\n",
    ")\n",
    "\n",
    "\n",
    "def save_metrics_results(df_summaries, model_name, results_path):\n",
    "    metrics_file_path = f\"{results_path}/csv/{model_name}_metrics_scores.csv\"\n",
    "    df_summaries.to_csv(metrics_file_path, index=False)\n",
    "    print(f\"Metrics results saved to {metrics_file_path}\")\n",
    "\n",
    "\n",
    "def plot_training_history(history, model_name, save_path):\n",
    "    plt.plot(history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Model Loss Over Epochs - {model_name}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(\n",
    "        os.path.join(save_path, f\"{model_name}_lossplot.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_model(model, model_name, save_path, save_full_model=True):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # Save the model weights\n",
    "    # model.save_weights(os.path.join(save_path, f\"{model_name}.weights.h5\"))\n",
    "    if save_full_model:\n",
    "        # Save the full model\n",
    "        model.save(os.path.join(save_path, f\"{model_name}_full_model.h5\"))\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_instance,\n",
    "    hyperparams,\n",
    "    x_training_padded,\n",
    "    y_training_padded,\n",
    "    x_validation_padded,\n",
    "    y_validation_padded,\n",
    "    save_path,\n",
    "):\n",
    "    K.clear_session()\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    latent_dim = hyperparams[\"latent_dim\"]\n",
    "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
    "    encoder_dropout = hyperparams[\"encoder_dropout\"]\n",
    "    encoder_recurrent_dropout = hyperparams[\"encoder_recurrent_dropout\"]\n",
    "    decoder_dropout = hyperparams[\"decoder_dropout\"]\n",
    "    decoder_recurrent_dropout = hyperparams[\"decoder_recurrent_dropout\"]\n",
    "    optimizer_class = hyperparams[\"optimizer_class\"]\n",
    "    epochs = hyperparams[\"epochs\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    learning_rate = hyperparams[\"learning_rate\"]\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "\n",
    "    # Set optimizer and callbacks\n",
    "    model_instance.change_optimizer(optimizer)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    def lr_schedule(epoch, lr):\n",
    "        decay_rate = 0.95\n",
    "        decay_step = 1\n",
    "        if epoch % decay_step == 0 and epoch != 0:\n",
    "            return lr * decay_rate\n",
    "        return lr\n",
    "\n",
    "    learning_rate_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "    # Reduce LR on Plateau\n",
    "    reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "\n",
    "    # Add callbacks to the model instance\n",
    "    model_instance.add_callbacks(\n",
    "        [early_stopping, learning_rate_scheduler, reduce_lr_on_plateau]\n",
    "    )\n",
    "\n",
    "    model = model_instance.get_model()\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        [x_training_padded, y_training_padded[:, :-1]],\n",
    "        y_training_padded.reshape(\n",
    "            y_training_padded.shape[0], y_training_padded.shape[1], 1\n",
    "        )[:, 1:],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(\n",
    "            [x_validation_padded, y_validation_padded[:, :-1]],\n",
    "            y_validation_padded.reshape(\n",
    "                y_validation_padded.shape[0], y_validation_padded.shape[1], 1\n",
    "            )[:, 1:],\n",
    "        ),\n",
    "        callbacks=model_instance.get_callbacks(),\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    TO_SAVE_MODEL = False\n",
    "    model_name = model_instance.name\n",
    "    model_save_path = os.path.join(save_path, \"weights\")\n",
    "    if TO_SAVE_MODEL:\n",
    "        save_model(model, model_name, model_save_path)\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        history.history, model_name, os.path.join(save_path, \"graphs\")\n",
    "    )\n",
    "\n",
    "    return history.history\n",
    "\n",
    "\n",
    "# Define hyperparameter grid\n",
    "hyperparameter_grid = create_hyperparameter_grid()\n",
    "\n",
    "# Define models\n",
    "model_classes = [\n",
    "    #Seq2SeqGRU,\n",
    "    #Seq2SeqLSTM,\n",
    "    Seq2SeqLSTMGlove,\n",
    "    Seq2SeqBiLSTM,\n",
    "    Seq2Seq3BiLSTM,\n",
    "    # Seq2SeqLSTMTransformer,\n",
    "]\n",
    "\n",
    "# Training loop\n",
    "results_path = f\"results/\"\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "for model_class in model_classes:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Training: {model_class.__name__}\")\n",
    "\n",
    "    results_path = f\"results/{model_class.__name__}\"\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    # Crea the subdirectories\n",
    "    os.makedirs(f\"{results_path}/weights\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/graphs\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/media/architectures\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/csv\", exist_ok=True)\n",
    "    os.makedirs(f\"{results_path}/histories\", exist_ok=True)\n",
    "\n",
    "    for hyperparams in hyperparameter_grid:\n",
    "        # Get prepared data\n",
    "        (\n",
    "            x_voc,\n",
    "            y_voc,\n",
    "            x_tokenizer,\n",
    "            y_tokenizer,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            max_text_len,\n",
    "            max_summary_len,\n",
    "        ) = prepare_data()\n",
    "\n",
    "        # Create the model instance\n",
    "        model_instance = model_class(\n",
    "            x_voc=x_voc,\n",
    "            y_voc=y_voc,\n",
    "            max_text_len=max_text_len,\n",
    "            max_summary_len=max_summary_len,\n",
    "            x_tokenizer=x_tokenizer,\n",
    "            y_tokenizer=y_tokenizer,\n",
    "            name_additional_info=f\"_optimizer{hyperparams['optimizer_class'].__name__}_lr{hyperparams['learning_rate']}_ed{hyperparams['embedding_dim']}_ld{hyperparams['latent_dim']}_do{hyperparams['decoder_dropout']}_drdo{hyperparams['decoder_recurrent_dropout']}_edo{hyperparams['encoder_dropout']}_erdo{hyperparams['encoder_recurrent_dropout']}\",\n",
    "            latent_dim=hyperparams[\"latent_dim\"],\n",
    "            embedding_dim=hyperparams[\"embedding_dim\"],\n",
    "            encoder_dropout=hyperparams[\"encoder_dropout\"],\n",
    "            encoder_recurrent_dropout=hyperparams[\"encoder_recurrent_dropout\"],\n",
    "            decoder_dropout=hyperparams[\"decoder_dropout\"],\n",
    "            decoder_recurrent_dropout=hyperparams[\"decoder_recurrent_dropout\"],\n",
    "        )\n",
    "\n",
    "        # Plot the model architecture\n",
    "        TO_SAVE_MODEL_ARCHITECTURE = False\n",
    "        if TO_SAVE_MODEL_ARCHITECTURE:\n",
    "            plot_model(\n",
    "                model_instance.get_model(),\n",
    "                to_file=f\"{results_path}/media/architectures/{model_instance.name}_architecture.png\",\n",
    "                show_shapes=True,\n",
    "            )\n",
    "\n",
    "        print(f\"Training {model_instance.name} with hyperparameters {hyperparams}\")\n",
    "        history = train_model(\n",
    "            model_instance,\n",
    "            hyperparams,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            x_validation_padded,\n",
    "            y_validation_padded,\n",
    "            results_path,\n",
    "        )\n",
    "\n",
    "        # Save training history\n",
    "        history_path = os.path.join(\n",
    "            results_path, f\"histories/{model_instance.name}_history.txt\"\n",
    "        )\n",
    "        with open(history_path, \"a\") as f:\n",
    "            f.write(f\"Hyperparameters: {hyperparams}\\n\")\n",
    "            f.write(f\"History: {history}\\n\\n\")\n",
    "            # Write last epoch loss, val_loss, accuracy, val_accuracy\n",
    "            f.write(\n",
    "                f\"Last epoch loss: {history['loss'][-1]}, val_loss: {history['val_loss'][-1]}\\n\"\n",
    "            )\n",
    "\n",
    "        # Generate and save summaries\n",
    "        print(f\"Generating summaries for {model_instance.name}\")\n",
    "        summaries_path = os.path.join(results_path, \"csv\")\n",
    "        df_summaries = generate_summaries(\n",
    "            model_instance,\n",
    "            x_training_padded,\n",
    "            y_training_padded,\n",
    "            max_text_len,\n",
    "            n_summaries=1000,\n",
    "            save_path=summaries_path,\n",
    "        )\n",
    "\n",
    "        df_summaries, mean_scores_rouge = evaluate_rouge(df_summaries)\n",
    "        df_summaries, mean_score_wer = evaluate_wer(df_summaries)\n",
    "        df_summaries, mean_score_cosine_similarity = evaluate_cosine_similarity(\n",
    "            df_summaries\n",
    "        )\n",
    "\n",
    "        # Save evaluation results\n",
    "        TO_SAVE_METRICS_RESULTS = True\n",
    "        if TO_SAVE_METRICS_RESULTS:\n",
    "            save_metrics_results(df_summaries, model_instance.name, results_path)\n",
    "\n",
    "        # Print mean scores in history file\n",
    "        with open(history_path, \"a\") as f:\n",
    "            f.write(f\"Mean ROUGE scores: {mean_scores_rouge}\\n\")\n",
    "            f.write(f\"Mean WER score: {mean_score_wer}\\n\")\n",
    "            f.write(f\"Mean Cosine Similarity score: {mean_score_cosine_similarity}\\n\\n\")\n",
    "\n",
    "        # Plot evaluation results\n",
    "        TO_SAVE_PLOTS = False\n",
    "\n",
    "        if TO_SAVE_PLOTS:\n",
    "            plot_rouge(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                metric=\"rouge1\",\n",
    "                title=f\"ROUGE-1 Score Distribution - {model_instance.name}\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "\n",
    "            plot_rouge(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                metric=\"rouge2\",\n",
    "                title=f\"ROUGE-2 Score Distribution - {model_instance.name}\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "            plot_rouge(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                metric=\"rougeL\",\n",
    "                title=f\"ROUGE-L Score Distribution - {model_instance.name}\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "            plot_wer(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                title=f\"WER Score Distribution - {model_instance.name}\",\n",
    "                color=\"red\",\n",
    "            )\n",
    "            plot_cosine_similarity(\n",
    "                df_summaries,\n",
    "                f\"{results_path}/media/graphs\",\n",
    "                model_instance,\n",
    "                title=f\"Cosine Similarity Distribution - {model_instance.name}\",\n",
    "                color=\"green\",\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
