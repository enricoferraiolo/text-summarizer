{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/enrico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: results/Seq2SeqBiLSTMImproved/weights/Seq2SeqBiLSTMImproved_optimizerAdam_lr0.001_ed300_ld256_do0.3_drdo0.3_edo0.3_erdo0.3_batch_size64_epochs50_full_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Import necessari\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from architectures.Seq2SeqGRU import Seq2SeqGRU\n",
    "from architectures.Seq2SeqLSTM import Seq2SeqLSTM\n",
    "from architectures.Seq2SeqLSTMGlove import Seq2SeqLSTMGlove\n",
    "from architectures.Seq2SeqBiLSTM import Seq2SeqBiLSTM\n",
    "from architectures.Seq2Seq3BiLSTM import Seq2Seq3BiLSTM\n",
    "from architectures.Seq2SeqLSTMTransformer import Seq2SeqLSTMTransformer\n",
    "from architectures.Seq2SeqBiLSTMImproved import Seq2SeqBiLSTMImproved\n",
    "from utils import prepare_data, generate_model_name_additional_info\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "optimizer_config = {\"class\": Adam, \"learning_rate\": 0.001}\n",
    "\n",
    "\n",
    "# Choose model to inference\n",
    "model_class = Seq2SeqBiLSTMImproved\n",
    "model_additional_info = {\n",
    "    \"additional_info\": \"\",\n",
    "    \"hyperparameters\": {\n",
    "        \"optimizer_class\": optimizer_config[\"class\"],\n",
    "        \"learning_rate\": optimizer_config[\"learning_rate\"],\n",
    "        \"embedding_dim\": 300,\n",
    "        \"latent_dim\": 256,\n",
    "        \"decoder_dropout\": 0.3,\n",
    "        \"decoder_recurrent_dropout\": 0.3,\n",
    "        \"encoder_dropout\": 0.3,\n",
    "        \"encoder_recurrent_dropout\": 0.3,\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "}\n",
    "# Build the model name\n",
    "model_info_name = model_class.__name__ + generate_model_name_additional_info(\n",
    "    model_additional_info.get(\"additional_info\"),\n",
    "    model_additional_info.get(\"hyperparameters\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Carica i dati e i tokenizer\n",
    "(\n",
    "    x_voc,\n",
    "    y_voc,\n",
    "    x_tokenizer,\n",
    "    y_tokenizer,\n",
    "    x_training_padded,\n",
    "    y_training_padded,\n",
    "    x_validation_padded,\n",
    "    y_validation_padded,\n",
    "    max_text_len,\n",
    "    max_summary_len,\n",
    ") = prepare_data()\n",
    "\n",
    "# Construct the model\n",
    "optimizer = model_additional_info[\"hyperparameters\"][\"optimizer_class\"](\n",
    "    learning_rate=model_additional_info[\"hyperparameters\"][\"learning_rate\"]\n",
    ")\n",
    "\n",
    "model_instance = model_class(\n",
    "    x_voc=x_voc,\n",
    "    y_voc=y_voc,\n",
    "    max_text_len=max_text_len,\n",
    "    max_summary_len=max_summary_len,\n",
    "    x_tokenizer=x_tokenizer,\n",
    "    y_tokenizer=y_tokenizer,\n",
    "    name_additional_info=generate_model_name_additional_info(\n",
    "        model_additional_info[\"additional_info\"],\n",
    "        model_additional_info[\"hyperparameters\"],\n",
    "    ),\n",
    "    latent_dim=model_additional_info[\"hyperparameters\"].get(\"latent_dim\"),\n",
    "    embedding_dim=model_additional_info[\"hyperparameters\"].get(\"embedding_dim\"),\n",
    "    encoder_dropout=model_additional_info[\"hyperparameters\"].get(\"encoder_dropout\"),\n",
    "    encoder_recurrent_dropout=model_additional_info[\"hyperparameters\"].get(\n",
    "        \"encoder_recurrent_dropout\"\n",
    "    ),\n",
    "    decoder_dropout=model_additional_info[\"hyperparameters\"].get(\"decoder_dropout\"),\n",
    "    decoder_recurrent_dropout=model_additional_info[\"hyperparameters\"].get(\n",
    "        \"decoder_recurrent_dropout\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "model_instance.change_optimizer(optimizer)\n",
    "model = model_instance.get_model()\n",
    "\n",
    "# Build the model name\n",
    "weights_model_file_name = model_info_name + \".h5\"\n",
    "full_weights_model_file_name = model_info_name + \"_full_model.h5\"\n",
    "\n",
    "TO_LOAD_FULL_MODEL = True\n",
    "model_file_name = \"\"\n",
    "if TO_LOAD_FULL_MODEL:\n",
    "    model_file_name = full_weights_model_file_name\n",
    "else:\n",
    "    model_file_name = weights_model_file_name\n",
    "\n",
    "# Load the model\n",
    "model_path = os.path.join(\"results\", model_class.__name__, \"weights\", model_file_name)\n",
    "model.load_weights(model_path)\n",
    "print(f\"Model loaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: daughter used quaker instant oatmeal liked lot long time ago tried version like fact want finish sure maybe lots new ingredients artificial\n",
      "Original summary: not very good\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "Predicted summary: \n",
      "\n",
      "\n",
      "Review: liked coffee much subscribing dark rich smooth\n",
      "Original summary: makes great cup of java\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Predicted summary: \n",
      "\n",
      "\n",
      "Review: like fool ordered box instead bag try disgusting smell like dead fish rinsed rinsed still smelled right light even look purple crack cannot believe stupid buy\n",
      "Original summary: gross\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Predicted summary: \n",
      "\n",
      "\n",
      "Review: flavorful packaged soup long time nice spicy taste good blend herbs cannot wait becomes available stores\n",
      "Original summary: delicious treat\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Predicted summary: \n",
      "\n",
      "\n",
      "Review: taste like pop refreshing mouth liked healthier pop great taste refreshing\n",
      "Original summary: very good\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Predicted summary: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_INFERENCES = 5\n",
    "\n",
    "for i in range(0, NUMBER_OF_INFERENCES):\n",
    "    print(\"Review:\", model_instance.seq2text(x_training_padded[i]))\n",
    "    print(\"Original summary:\", model_instance.seq2summary(y_training_padded[i]))\n",
    "    print(\"Predicted summary:\", model_instance.decode_sequence(x_training_padded[i].reshape(1, max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
