\input{sections/architetture/utils.tex}

\section{Model Architectures}
For this project, I chose to compare different models by implementing various \textbf{Sequence-to-Sequence} architectures using both \textbf{LSTM} and \textbf{GRU} layers.\\
The implementation was carried out using an abstract class called \texttt{BaseModel}, from which all specific model classes are derived.\\
This approach defines a common interface for all summarization models and allows for future architectural extensions with ease.\\

\subsection{Abstract Base Class}
The \texttt{BaseModel} class provides the base interface for all summarization models:
\begin{itemize}
    \item Abstract methods for building the encoder and decoder.
    \item Functionality for saving, loading, and performing inference with the model.
    \item Token-to-text and text-to-token conversion using the appropriate tokenizers.
\end{itemize}

\subsection{Training}
The training of models derived from the \texttt{BaseModel} class was carried out using the preprocessed dataset.\\
Before starting the training, the dataset was split into a training set and a validation set, with a ratio of 90\% and 10\% respectively.\\
Then I moved on to the actual training phase of the models, using the loss function \texttt{Sparse Categorical Crossentropy}, which is useful in summarization tasks.\\

\subsection{Hyperparameters}
For each model class, training was performed multiple times, each with a different combination of hyperparameters.\\
These combinations are generated through a \textit{hyperparameter\_grid}
implemented in the function \texttt{create\_hyperparameter\_grid}, which returns all the
possible permutations of the values provided for the following parameters:
\begin{itemize}
    \item \texttt{embedding\_dim}: dimension of the word embedding
    \item \texttt{hidden\_dim}: dimension of the hidden state of the encoder and decoder
    \item \texttt{encoder\_dropout}: dropout rate of the encoder
    \item \texttt{encoder\_recurrent\_dropout}: dropout rate for the recurrent states of the encoder
    \item \texttt{decoder\_dropout}: dropout rate of the decoder
    \item \texttt{decoder\_recurrent\_dropout}: dropout rate for the recurrent states of the decoder
    \item \texttt{optimizer}: optimizer to use during training
    \item \texttt{learning\_rate}: learning rate
    \item \texttt{batch\_size}: batch size during training
    \item \texttt{epochs}: number of epochs for training
\end{itemize}

For each \textbf{hyperparameter permutation}, the following training pipeline was executed:
\begin{enumerate}
    \item \textbf{Data Preparation}: loading and preprocessing of the data.
    \item \textbf{Model Instantiation}: initializing the current model class with the selected hyperparameters.
    \item \textbf{Model Compilation}: compiling the model and starting training with various callbacks (discussed in the next section).
    \item \textbf{Model Training}: executing the training process.
    \item \textbf{Saving Results}:
          \begin{itemize}
              \item \textit{Model weights} (\texttt{result/\{model\_class\}/weights/})
              \item \textit{Model architecture} (\texttt{result/\{model\_class\}/media/architectures/})
              \item \textit{Loss plot} (\texttt{result/\{model\_class\}/media/graphs/})
              \item \textit{Training history} (\texttt{result/\{model\_class\}/histories/})
              \item \textit{Generated summaries}: at the end of training, summaries were generated from validation reviews and saved in a CSV file (\texttt{result/\{model\_class\}/csv/})
          \end{itemize}
\end{enumerate}

\subsubsection{Callback}
During training, I also used the following callback functions:
\begin{itemize}
    \item \textbf{Early Stopping}: monitors a metric, in this case the validation loss, and stops training if there are no improvements for a certain number of consecutive epochs. This helps prevent overfitting and saves computation time.
    \item \textbf{Learning Rate Scheduler}: adjusts the learning rate during training according to a strategy, in my case I used \texttt{Step Decay}, which reduces the learning rate by a fixed factor every few epochs.
    \item \textbf{Reduce LR on Plateau}: monitors a metric, in this case the validation loss, and reduces the learning rate if there are no improvements for a certain number of consecutive epochs. This helps optimize the training process and find a more effective learning rate.
\end{itemize}
In this way, I was able to achieve the best results for each model by adjusting its hyperparameters and trying different combinations.

\subsection{Tested Architectures}
Several summarization model architectures were tested, each with different characteristics and parameter configurations.\\
The two main categories of implemented models are:
\begin{itemize}
    \item \textbf{LSTM}: models based on LSTM layers for both encoder and decoder.
    \item \textbf{GRU}: models based on GRU layers for both encoder and decoder.
\end{itemize}
These architectures are based on RNNs (Recurrent Neural Networks) and were chosen for their effectiveness in text summarization tasks, as they handle dependencies between words in text sequences well.\\
\begin{itemize}
    \item \textbf{LSTM}: Long Short-Term Memory, is a variant of RNNs that solves the vanishing gradient problem, thanks to the presence of a long-term memory mechanism.
          This gating mechanism allows it to store important information and discard less relevant data.
    \item \textbf{GRU}: Gated Recurrent Unit, is a simpler variant of LSTMs, with fewer parameters and less computational complexity.
          Again, the gating mechanism allows it to store important information and discard less relevant data.
\end{itemize}

In order to make the reading smoother, only the best results obtained during training with the best parameters and configurations found (based on the \texttt{BERT score}, which will be discussed later) are reported for each class, although numerous attempts and tests were conducted, which are reported in the following sections in a comparative table.\\


\input{sections/architetture/Seq2SeqLSTM.tex}
\input{sections/architetture/Seq2SeqBiLSTM.tex}
\input{sections/architetture/Seq2Seq3BiLSTM.tex}
%\input{sections/architetture/Seq2SeqLSTMGlove.tex}
\input{sections/architetture/Seq2SeqGRU.tex}