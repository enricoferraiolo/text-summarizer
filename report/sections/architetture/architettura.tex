\input{sections/architetture/utils.tex}

\section{Architettura dei Modelli}
Per svolgere questo progetto ho deciso di effettuare un confronto tra diversi modelli, implementando architetture Seq2Seq con LSTM e GRU.\\
L'implementazione dei modelli è stata effettuata attraverso una classe astratta \texttt{BaseModel} e la successiva creazioni e implementazione di classi derivate.\\
Questo permette di definire un'interfaccia comune per tutti i modelli di summarization e di estendere facilmente l'architettura in futuro.\\

\subsection{Classe Base Astratta}
La classe \texttt{BaseModel} fornisce l'interfaccia base per tutti i modelli di summarization:
\begin{itemize}
    \item Metodi astratti per costruire encoder e decoder.
    \item Funzionalità per il salvataggio, caricamento e inferenza del modello.
    \item Conversione tra sequenze e testo tramite i tokenizer.
\end{itemize}

\subsection{Training}
L'addestramento dei modelli, derivati dalla classe \texttt{BaseModel}, è stato effettuato utilizzando il dataset preprocessato.\\
Prima di iniziare l'addestramento, il dataset è stato suddiviso in training set e validation set, con una proporzione del 90\% e 10\% rispettivamente.\\
Dopodiché sono passato alla fase effettiva di training dei modelli, utilizzando e la loss function \texttt{Sparse Categorical Crossentropy}, utile nei task di summarization.\\

\subsubsection{Callbacks}
Durante il training ho utilizzato anche le seguenti funzioni di callback:
\begin{itemize}
    \item \textbf{Early Stopping}: monitora una metrica, in questo caso la validation loss, e interrompe l'addestramento se non ci sono miglioramenti per un certo numero di epoche consecutive. Questo aiuta a prevenire l'overfitting e a risparmiare tempo di calcolo.
    \item \textbf{Learning Rate Scheduler}: regola il tasso di apprendiento durante il training secondo una strategia, nel mio caso ho utilizzato la \texttt{Step Decay}, che riduce il learning rate di un fattore fisso ogni tot epoche.
    \item \textbf{Reduce LR on Plateau}: monitora una metrica, in questo caso la validation loss, e riduce il learning rate se non ci sono miglioramenti per un certo numero di epoche consecutive. Questo aiuta a ottimizzare il processo di addestramento e a trovare un tasso di apprendimento più efficace.
\end{itemize}

\subsection{Architetture sperimentate}
Sono state sperimentante diverse architetture di modelli di summarization, ognuna con caratteristiche e parametri diversi.\\
Le due categorie principali di modelli implementati sono:
\begin{itemize}
    \item \textbf{LSTM}: modelli basati su layer LSTM per encoder e decoder.
    \item \textbf{GRU}: modelli basati su layer GRU per encoder e decoder.
\end{itemize}
Tali architetture sono basate sulle RNN (Recurrent Neural Networks) e sono state scelte per la loro efficacia nei task di text-summarization, poiché 
gestiscono le dipendenze tra le parole su sequenze di testo.\\ 
\begin{itemize}
    \item \textbf{LSTM}: Long Short-Term Memory, è una variante delle RNN che risolve il problema del vanishing gradient, grazie alla presenza di un meccanismo di memoria a lungo termine.
    Tale meccanismo di gating permette di memorizzare informazioni importanti e scartare quelle meno rilevanti.
    \item \textbf{GRU}: Gated Recurrent Unit, è una variante più semplice delle LSTM, con meno parametri e meno complessità computazionale.
    Anche in questo caso, il meccanismo di gating permette di memorizzare informazioni importanti e scartare quelle meno rilevanti.
\end{itemize}

Al fine di rendere più scorrevole la lettura, per ogni classe vengono riportati solamente i migliori risultati ottenuti durante l'addestramento con i migliori parametri e le migliori configurazioni trovate, sebbene
siano stati effettuati numerosi tentativi e test riportati in seguito in una tabella comparativa.\\


\input{sections/architetture/Seq2SeqLSTM.tex}
\input{sections/architetture/Seq2SeqBiLSTM.tex}
\input{sections/architetture/Seq2Seq3BiLSTM.tex}
\input{sections/architetture/Seq2SeqLSTMGlove.tex}
\input{sections/architetture/Seq2SeqGRU.tex}